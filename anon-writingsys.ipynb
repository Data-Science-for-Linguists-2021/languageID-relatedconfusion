{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-feeding",
   "metadata": {},
   "source": [
    "# Anonymizing languages' writing systems\n",
    "For each language's extracted text file (or more precisely a portion I have chosen for the train dataset), I compute how common each character is. The most common character corresponds to 0, second most common to 1 and so on. The space character is not transformed; it's still just space. Then, I replace the characters with their number correspondances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "confidential-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sys # to get max int\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "respiratory-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"Hi-5 were an Australian children's musical group formed in 1998 in association with the children's television series of the same name, which premiered on the Nine Network in 1999. The five performers entertained and educated preschool children through music, movement and play. Kellie Crawford, Kathleen de Leon Jones, Nathan Foley, Tim Harding and Charli Robinson were the founding members. As one of Australia's highest-paid entertainment groups, they were placed in Business Review Weekly's annual list several times. The Australian Recording Industry Association certified one of their albums, It's a Party, as double platinum; Jump and Jive with Hi-5, Boom Boom Beat, and It's a Hi-5 Christmas were certified platinum. By 2004, the original line-up had received three Logie Television Awards for Most Outstanding Children's Program and five consecutive ARIA Music Awards for Best Children's Album.\"\"\"\n",
    "char_dict = {}\n",
    "char_dict[' '] = 0 # 0 is reserved character for space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aggregate-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in s:\n",
    "    if c == ' ' or c == '\\n':\n",
    "        continue\n",
    "    char_dict[c] = char_dict.get(c, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "heated-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(char_dict, orient='index')\n",
    "df.sort_values(by=0, inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "yellow-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "utility-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "color-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('index', inplace=True)\n",
    "del df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "relevant-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = df.to_dict()['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "detailed-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2arr = list(s)\n",
    "for i,c in enumerate(s2arr):\n",
    "    s2arr[i] = trans.get(c, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "liable-howard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in s2arr if type(c) is not int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "diverse-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = './data_chunked/'\n",
    "outpath = './data/chunks-anon/'\n",
    "files = ['en.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "noble-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(chunks):\n",
    "    char_dict = {}\n",
    "    char_dict[' '] = sys.maxsize\n",
    "    # index 0 is reserved character for space, corresponds to \"most common\"\n",
    "    \n",
    "    for s in chunks:\n",
    "        for c in s: # iterate over ea char in ea chunk\n",
    "            if c == ' ' or c == '\\n':\n",
    "                continue\n",
    "            char_dict[c] = char_dict.get(c, 0) + 1 # find num occurences of ea char\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(char_dict, orient='index', columns=['occur'])\n",
    "    df.sort_values(by='occur', inplace=True, ascending=False) # sort by num occurrences\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['char', 'occur']\n",
    "    df['ind'] = df.index # space will be index 0, most common char index 1, etc\n",
    "    df.set_index('char', inplace=True) # set index back to the characters\n",
    "    del df['occur'] # delete the num occurences\n",
    "    trans = df.to_dict()['ind'] # dict of char to word common-ness\n",
    "    return df, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "precious-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(chunks, trans):\n",
    "    chunks_trans = []\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        chunkarr = list(chunk)[:-1] # strip off newline \\n char at very end\n",
    "        for j,c in enumerate(chunkarr):\n",
    "            chunkarr[j] = trans[c]\n",
    "        chunks_trans.append(chunkarr)\n",
    "    \n",
    "    return chunks_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "inside-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaa.txt\n",
      "glk.txt\n",
      "el.txt\n",
      "gd.txt\n",
      "io.txt\n",
      "myv.txt\n",
      "sv.txt\n",
      "sa.txt\n",
      "sw.txt\n",
      "pl.txt\n",
      "mni.txt\n",
      "fa.txt\n",
      "bjn.txt\n",
      "eml.txt\n",
      "koi.txt\n",
      "kab.txt\n",
      "rue.txt\n",
      "eo.txt\n",
      "mzn.txt\n",
      "tcy.txt\n",
      "su.txt\n",
      "diq.txt\n",
      "sc.txt\n",
      "shn.txt\n",
      "azb.txt\n",
      "ang.txt\n",
      "ja.txt\n",
      "hi.txt\n",
      "jv.txt\n",
      "tyv.txt\n",
      "en.txt\n",
      "gor.txt\n",
      "gan.txt\n",
      "lez.txt\n",
      "lij.txt\n",
      "pag.txt\n",
      "pap.txt\n",
      "pms.txt\n",
      "mwl.txt\n",
      "gu.txt\n",
      "ka.txt\n",
      "kv.txt\n",
      "awa.txt\n",
      "sq.txt\n",
      "ru.txt\n",
      "kw.txt\n",
      "ace.txt\n",
      "nqo.txt\n",
      "ga.txt\n",
      "gv.txt\n",
      "fr.txt\n",
      "hy.txt\n",
      "ku.txt\n",
      "sd.txt\n",
      "skr.txt\n",
      "rw.txt\n",
      "sr.txt\n",
      "se.txt\n",
      "hsb.txt\n",
      "vls.txt\n",
      "xal.txt\n",
      "inh.txt\n",
      "udm.txt\n",
      "lad.txt\n",
      "krc.txt\n",
      "co.txt\n",
      "ms.txt\n",
      "zea.txt\n",
      "tg.txt\n",
      "jbo.txt\n",
      "vo.txt\n",
      "min.txt\n",
      "ceb.txt\n",
      "la.txt\n",
      "lv.txt\n",
      "om.txt\n",
      "mr.txt\n",
      "cy.txt\n",
      "af.txt\n",
      "nap.txt\n",
      "war.txt\n",
      "wuu.txt\n",
      "as.txt\n",
      "bh.txt\n",
      "lt.txt\n",
      "mg.txt\n",
      "sat.txt\n",
      "tr.txt\n",
      "te.txt\n",
      "ts.txt\n",
      "dty.txt\n",
      "lb.txt\n",
      "crh.txt\n",
      "ar.txt\n",
      "szl.txt\n",
      "av.txt\n",
      "ny.txt\n",
      "tpi.txt\n",
      "nn.txt\n",
      "scn.txt\n",
      "ary.txt\n",
      "hyw.txt\n",
      "ur.txt\n",
      "vi.txt\n",
      "ta.txt\n",
      "sco.txt\n",
      "stq.txt\n",
      "mt.txt\n",
      "no.txt\n",
      "lg.txt\n",
      "ban.txt\n",
      "ab.txt\n",
      "bn.txt\n",
      "zh.txt\n",
      "ug.txt\n",
      "arz.txt\n",
      "wo.txt\n",
      "tt.txt\n",
      "nl.txt\n",
      "bo.txt\n",
      "szy.txt\n",
      "README.md\n",
      "bug.txt\n",
      "an.txt\n",
      "ay.txt\n",
      "li.txt\n",
      "or.txt\n",
      "nv.txt\n",
      "uk.txt\n",
      "tn.txt\n",
      "yi.txt\n",
      "cdo.txt\n",
      "sah.txt\n",
      "ml.txt\n",
      "os.txt\n",
      "lmo.txt\n",
      "pfl.txt\n",
      "am.txt\n",
      "az.txt\n",
      "ba.txt\n",
      "ce.txt\n",
      "mn.txt\n",
      "my.txt\n",
      "tl.txt\n",
      "ckb.txt\n",
      "wa.txt\n",
      "hak.txt\n",
      "cs.txt\n",
      "vep.txt\n",
      "bcl.txt\n",
      "ksh.txt\n",
      "vec.txt\n",
      "mrj.txt\n",
      "bs.txt\n",
      "oc.txt\n",
      "mk.txt\n",
      "lo.txt\n",
      "zu.txt\n",
      "uz.txt\n",
      "ast.txt\n",
      "fur.txt\n",
      "th.txt\n",
      "mdf.txt\n",
      "yo.txt\n",
      "mhr.txt\n",
      "ln.txt\n",
      "cv.txt\n",
      "br.txt\n",
      "ca.txt\n",
      "be.txt\n",
      "nah.txt\n",
      "bar.txt\n",
      "nov.txt\n",
      "bxr.txt\n",
      "bg.txt\n",
      "csb.txt\n",
      "smn.txt\n",
      "tk.txt\n",
      "tet.txt\n",
      "xh.txt\n",
      "mi.txt\n",
      "ne.txt\n",
      "pdc.txt\n",
      "lbe.txt\n",
      "nrm.txt\n",
      "kbd.txt\n",
      "fi.txt\n",
      "dv.txt\n",
      "da.txt\n",
      "kn.txt\n",
      "hu.txt\n",
      "als.txt\n",
      "dsb.txt\n",
      "ky.txt\n",
      "sh.txt\n",
      "ps.txt\n",
      "si.txt\n",
      "frr.txt\n",
      "rm.txt\n",
      "avk.txt\n",
      "mnw.txt\n",
      "ig.txt\n",
      "ht.txt\n",
      "ko.txt\n",
      "gl.txt\n",
      "es.txt\n",
      "kbp.txt\n",
      "gom.txt\n",
      "gn.txt\n",
      "km.txt\n",
      "ha.txt\n",
      "ie.txt\n",
      "frp.txt\n",
      "sk.txt\n",
      "ro.txt\n",
      "qu.txt\n",
      "id.txt\n",
      "is.txt\n",
      "gag.txt\n",
      "ilo.txt\n",
      "simple.txt\n",
      "nso.txt\n",
      "ext.txt\n",
      "bpy.txt\n",
      "et.txt\n",
      "fo.txt\n",
      "sn.txt\n",
      "pt.txt\n",
      "so.txt\n",
      "mai.txt\n",
      "hr.txt\n",
      "ia.txt\n",
      "he.txt\n",
      "eu.txt\n",
      "hif.txt\n",
      "fy.txt\n",
      "pam.txt\n",
      "pnb.txt\n",
      "it.txt\n",
      "kk.txt\n",
      "pa.txt\n",
      "sl.txt\n",
      "xmf.txt\n",
      "de.txt\n",
      "olo.txt\n",
      "new.txt\n",
      "nds.txt\n",
      "pcd.txt\n",
      "lfn.txt\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(inpath):\n",
    "    if f == 'README.md' or f.startswith('.'): continue\n",
    "    print(f)\n",
    "    file = open(inpath+f, 'r')\n",
    "    chunks = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    df, trans = make_transform(chunks)\n",
    "    chunks_trans = apply_transform(chunks, trans)\n",
    "    \n",
    "    file = open(outpath+f, 'wb')\n",
    "    pickle.dump(chunks_trans, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
