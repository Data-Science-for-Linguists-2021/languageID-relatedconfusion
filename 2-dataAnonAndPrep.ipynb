{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absolute-storm",
   "metadata": {},
   "source": [
    "# Final data prep\n",
    "\n",
    "(NEW CONTINUING)\n",
    "\n",
    "This is the second notebook in the project\n",
    "\n",
    "Basically, all the data was gathered already (that process was shown in the previous notebook). So, this code takes care of nearly everything in between the data gathering and the actual machine learning. I create a version of the data with the languages' writing systems anonymized, and export both the anonymized and non-anonymized data to files that can be read in in the ML notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-going",
   "metadata": {},
   "source": [
    "## Tabel of contents\n",
    "- [Anonymizing languages' writing systems](#Anonymizing-languages'-writing-systems)\n",
    "- [Prepare data for ML](#Prepare-data-for-ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clinical-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sys # to get max int\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-syndicate",
   "metadata": {},
   "source": [
    "## Anonymizing languages' writing systems\n",
    "\n",
    "For each language's extracted text file (or more precisely a portion I have chosen for the train dataset), I compute how common each character is. The most common character corresponds to 0, second most common to 1 and so on. The space character is not transformed; it's still just space. Then, I replace the characters with their number correspondances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sudden-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = './data/chunks-nonanon/'\n",
    "outpath = './data/chunks-anon/'\n",
    "files = ['hr.txt'] #sample list for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exclusive-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigns each character to a number. space is always 0. then, if the most common non-space\n",
    "# char is found to be 'e', then e gets assigned 1. if 't' is second most common, t is given 2.\n",
    "def make_transform(chunks):\n",
    "    char_dict = {}\n",
    "    char_dict[' '] = sys.maxsize\n",
    "    # index 0 is reserved character for space, corresponds to \"most common\"\n",
    "    # you get ind 0 by being having the greatest occurences, so that's max_int\n",
    "    \n",
    "    for s in chunks:\n",
    "        for c in s: # iterate over ea char in ea chunk\n",
    "            if c == ' ' or c == '\\n':\n",
    "                continue\n",
    "            char_dict[c] = char_dict.get(c, 0) + 1 # find num occurences of ea char\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(char_dict, orient='index', columns=['occur'])\n",
    "    df.sort_values(by='occur', inplace=True, ascending=False) # sort by num occurrences\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['char', 'occur']\n",
    "    df['ind'] = df.index # space will be index 0, most common char index 1, etc\n",
    "    df = df[df['ind']<256] # TEMPORARY FIX\n",
    "    df.set_index('char', inplace=True) # set index back to the characters\n",
    "    del df['occur'] # delete the num occurences\n",
    "    trans = df.to_dict()['ind']\n",
    "    return df, trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expressed-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the supplied transformation, transform ea char in the text\n",
    "def apply_transform(chunks, trans):\n",
    "    chunks_trans = []\n",
    "    \n",
    "    for i,chunk in enumerate(chunks):\n",
    "        chunkarr = list(chunk)[:-1] # strip off newline \\n char at very end\n",
    "        for j,c in enumerate(chunkarr):\n",
    "            chunkarr[j] = trans.get(c, 255)\n",
    "        chunks_trans.append(bytes(chunkarr))\n",
    "    \n",
    "    return chunks_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "complimentary-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over ea lang, make and apply transform, save to file\n",
    "for f in os.listdir(inpath):\n",
    "    if f == 'README.md' or f.startswith('.'): continue\n",
    "#     print(f)\n",
    "    file = open(inpath+f, 'r')\n",
    "    chunks = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    df, trans = make_transform(chunks)\n",
    "    chunks_trans = apply_transform(chunks, trans)\n",
    "\n",
    "    file = open(outpath+f, 'wb')\n",
    "    pickle.dump(chunks_trans, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minor-graduation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x0c\\x06\\x05\\x00\\x02\\x03\\t\\x01\\x00\\x08\\x02\\x05\\x01\\x03\\x00\\x07\\x01\\x00\\x04\\x0e\\r\\x02\\x08\\x06\\x03\\x00\\x12\\x08\\x04\\t\\x02\\x03\\x00\\x02\\x00\\x04\\t\\x01\\x07\\x04\\x01\\x05\\x00\\x04\\x01\\x00\\x10\\x01\\x05\\x04\\x01\\x00\\x0c\\x06\\x05\\t\\x08\\x06\\x07\\x01\\x00\\n\\x02\\x00\\r\\x01\\x08\\t\\x02\\x03\\x00\\n\\x02\\x00\\x07\\x01\\x00\\x0c\\x06\\x03\\t\\x01\\x00\\x02\\x00\\x04\\x01\\x00\\x04\\x05\\x03\\t\\x04\\t\\x0b\\x04\\x00\\x07\\x01\\x00\\x0c\\x06\\x07\\x06\\x05\\x04\\x01\\x03\\x00\\x12\\x08\\x04\\t\\x02\\x03\\x00\\x02\\x00\\x04\\t\\x01\\x07\\x04\\x01\\x05\\x00\\n\\x02\\x00\\x17\\x06\\x0e\\x01\\x07\\x04\\x07\\x01\\x05\\n\\x00\\x1f\\x02\\x00\\x17\\x06\\x0e\\x01\\x07\\x04\\x01\\x00\\x05\\x06\\x00\\x04\\x01\\x00\\x01\\x0f\\x02\\x00\\x0b\\x05\\x00\\x10\\x06\\x0f\\x02\\x08\\x05\\x01\\x00\\x0c\\x0b\\x01\\x07\\x00\\x0c\\x06\\x05\\t\\x08\\x06\\x07\\x01\\x00\\x07\\x01\\x00\\r\\x01\\x04\\x03\\x00\\x04\\x05\\t\\x02\\x08\\x01\\x00\\x15\\x05\\x00\\x07\\x01\\x00\\x05\\x06\\x08\\n\\x02\\x00\\x07\\x01\\x00\\x08\\x02\\x13\\x04\\x06\\x05\\x00\\n\\x02\\x00\\x17\\x06\\x0e\\x01\\x07\\x04\\x07\\x01\\x05\\n\\x00\\x04\\x01\\x00\\n\\x02\\x0c\\x07\\x01\\x08\\x01\\x00\\x01\\x0b\\t\\x06\\x05\\x06\\x0e\\x04\\x01\\x00\\x02\\x05\\x00\\x01\\x0c\\x02\\x07\\x00\\x01\\x05\\x04\\x06\\x00\\x1a\\x0b\\x05\\t\\x07\\x01\\x05\\n\\x00\\x02\\x03\\x00\\x0b\\x05\\x00\\x06\\t\\x08\\x01\\x00\\x08\\x02\\x13\\x04\\x06\\x05\\x00\\x0c\\x0b\\x01\\x07\\x00\\x04\\x01\\x00\\n\\x02\\x0c\\x07\\x01\\x08\\x01\\x00\\x05\\x06\\x05\\n\\x02\\r\\x02\\x05\\n\\x02\\x00\\x17\\x06\\x0e\\x01\\x07\\x04\\x01\\x00\\x01\\x0f\\x02\\x00\\x0e\\x04\\x07\\x04\\x06\\x05\\x00\\r\\x02\\x08\\x03\\x06\\x05\\x02\\x03\\x00\\x1b\\x0b\\x07\\t\\x02\\x00\\x02\\x03\\x00\\x05\\x06\\x0e\\x01\\n\\x01\\x00\\x17\\x0b\\x01\\x00\\x03\\x04\\t\\x02\\x00\\x07\\x01\\x00\\r\\x07\\x0b\\x00\\x10\\x08\\x01\\x05\\n\\x02\\x00\\x02\\x03\\x00\\x07\\x01\\x00\\x0c\\x01\\r\\x04\\t\\x01\\x07\\x00\\x1b\\x06\\x10\\x01\\n\\x04\\x03\\x19\\x0b\\x00$\\x08\\x01\\x00\\x03\\x02\\x05\\t\\x02\\x05\\x04\\x06\\x03\\x00\\x07\\x01\\x00\\x03\\x06\\x0e\\x01\\x07\\x04\\x03\\x00\\x04\\x01\\x00\\r\\x08\\x06\\n\\x0b\\x04\\x00\\x06\\x12\\x08\\x01\\x03\\x00\\x05\\x06\\t\\x01\\x12\\x07\\x02\\x00\\n\\x02\\x00\\x07\\x02\\t\\x02\\x08\\x01\\t\\x0b\\x08\\x00\\x0e\\x0b\\x03\\x07\\x04\\x0e\\x00\\x18\\x06\\x05\\x00\\x07\\x01\\x00\\x01\\n\\x06\\t\\x01\\x00\\n\\x02'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_trans[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-hamilton",
   "metadata": {},
   "source": [
    "## Prepare data for ML\n",
    "Each language is represented in its own file. This code reads all the files and puts them into one dataframe. Works for A or NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eligible-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = './data/chunks-anon/'\n",
    "outpath= './data/'\n",
    "dfs = []\n",
    "data = pd.DataFrame([], columns=['text', 'lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beautiful-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaa.txt\n",
      "glk.txt\n",
      "el.txt\n",
      "gd.txt\n",
      "io.txt\n",
      "myv.txt\n",
      "sv.txt\n",
      "sa.txt\n",
      "sw.txt\n",
      "pl.txt\n",
      "mni.txt\n",
      "fa.txt\n",
      "bjn.txt\n",
      "eml.txt\n",
      "koi.txt\n",
      "kab.txt\n",
      "rue.txt\n",
      "eo.txt\n",
      "mzn.txt\n",
      "tcy.txt\n",
      "su.txt\n",
      "diq.txt\n",
      "sc.txt\n",
      "shn.txt\n",
      "azb.txt\n",
      "ang.txt\n",
      "ja.txt\n",
      "hi.txt\n",
      "jv.txt\n",
      "tyv.txt\n",
      "en.txt\n",
      "gor.txt\n",
      "gan.txt\n",
      "lez.txt\n",
      "lij.txt\n",
      "pag.txt\n",
      "pap.txt\n",
      "pms.txt\n",
      "mwl.txt\n",
      "gu.txt\n",
      "ka.txt\n",
      "kv.txt\n",
      "awa.txt\n",
      "sq.txt\n",
      "ru.txt\n",
      "kw.txt\n",
      "ace.txt\n",
      "nqo.txt\n",
      "ga.txt\n",
      "gv.txt\n",
      "fr.txt\n",
      "hy.txt\n",
      "ku.txt\n",
      "sd.txt\n",
      "skr.txt\n",
      "rw.txt\n",
      "sr.txt\n",
      "se.txt\n",
      "hsb.txt\n",
      "vls.txt\n",
      "xal.txt\n",
      "inh.txt\n",
      "udm.txt\n",
      "lad.txt\n",
      "krc.txt\n",
      "co.txt\n",
      "ms.txt\n",
      "zea.txt\n",
      "tg.txt\n",
      "jbo.txt\n",
      "vo.txt\n",
      "min.txt\n",
      "ceb.txt\n",
      "la.txt\n",
      "lv.txt\n",
      "om.txt\n",
      "mr.txt\n",
      "cy.txt\n",
      "af.txt\n",
      "nap.txt\n",
      "war.txt\n",
      "wuu.txt\n",
      "as.txt\n",
      "bh.txt\n",
      "lt.txt\n",
      "mg.txt\n",
      "sat.txt\n",
      "tr.txt\n",
      "te.txt\n",
      "ts.txt\n",
      "dty.txt\n",
      "lb.txt\n",
      "crh.txt\n",
      "ar.txt\n",
      "szl.txt\n",
      "av.txt\n",
      "ny.txt\n",
      "tpi.txt\n",
      "nn.txt\n",
      "scn.txt\n",
      "ary.txt\n",
      "hyw.txt\n",
      "ur.txt\n",
      "vi.txt\n",
      "ta.txt\n",
      "sco.txt\n",
      "stq.txt\n",
      "mt.txt\n",
      "no.txt\n",
      "lg.txt\n",
      "ban.txt\n",
      "ab.txt\n",
      "bn.txt\n",
      "zh.txt\n",
      "ug.txt\n",
      "arz.txt\n",
      "wo.txt\n",
      "tt.txt\n",
      "nl.txt\n",
      "bo.txt\n",
      "szy.txt\n",
      "bug.txt\n",
      "an.txt\n",
      "ay.txt\n",
      "li.txt\n",
      "or.txt\n",
      "nv.txt\n",
      "uk.txt\n",
      "tn.txt\n",
      "yi.txt\n",
      "cdo.txt\n",
      "sah.txt\n",
      "ml.txt\n",
      "os.txt\n",
      "lmo.txt\n",
      "pfl.txt\n",
      "am.txt\n",
      "az.txt\n",
      "ba.txt\n",
      "ce.txt\n",
      "mn.txt\n",
      "my.txt\n",
      "tl.txt\n",
      "ckb.txt\n",
      "wa.txt\n",
      "hak.txt\n",
      "cs.txt\n",
      "vep.txt\n",
      "bcl.txt\n",
      "ksh.txt\n",
      "vec.txt\n",
      "mrj.txt\n",
      "bs.txt\n",
      "oc.txt\n",
      "mk.txt\n",
      "lo.txt\n",
      "zu.txt\n",
      "uz.txt\n",
      "ast.txt\n",
      "fur.txt\n",
      "th.txt\n",
      "mdf.txt\n",
      "yo.txt\n",
      "mhr.txt\n",
      "ln.txt\n",
      "cv.txt\n",
      "br.txt\n",
      "ca.txt\n",
      "be.txt\n",
      "nah.txt\n",
      "bar.txt\n",
      "nov.txt\n",
      "bxr.txt\n",
      "bg.txt\n",
      "csb.txt\n",
      "smn.txt\n",
      "tk.txt\n",
      "tet.txt\n",
      "xh.txt\n",
      "mi.txt\n",
      "ne.txt\n",
      "pdc.txt\n",
      "lbe.txt\n",
      "nrm.txt\n",
      "kbd.txt\n",
      "fi.txt\n",
      "dv.txt\n",
      "da.txt\n",
      "kn.txt\n",
      "hu.txt\n",
      "als.txt\n",
      "dsb.txt\n",
      "ky.txt\n",
      "sh.txt\n",
      "ps.txt\n",
      "si.txt\n",
      "frr.txt\n",
      "rm.txt\n",
      "avk.txt\n",
      "mnw.txt\n",
      "ig.txt\n",
      "ht.txt\n",
      "ko.txt\n",
      "gl.txt\n",
      "es.txt\n",
      "kbp.txt\n",
      "gom.txt\n",
      "gn.txt\n",
      "km.txt\n",
      "ha.txt\n",
      "ie.txt\n",
      "frp.txt\n",
      "sk.txt\n",
      "ro.txt\n",
      "qu.txt\n",
      "id.txt\n",
      "is.txt\n",
      "gag.txt\n",
      "ilo.txt\n",
      "simple.txt\n",
      "nso.txt\n",
      "ext.txt\n",
      "bpy.txt\n",
      "et.txt\n",
      "fo.txt\n",
      "sn.txt\n",
      "pt.txt\n",
      "so.txt\n",
      "mai.txt\n",
      "hr.txt\n",
      "ia.txt\n",
      "he.txt\n",
      "eu.txt\n",
      "hif.txt\n",
      "fy.txt\n",
      "pam.txt\n",
      "pnb.txt\n",
      "it.txt\n",
      "kk.txt\n",
      "pa.txt\n",
      "sl.txt\n",
      "xmf.txt\n",
      "de.txt\n",
      "olo.txt\n",
      "new.txt\n",
      "nds.txt\n",
      "pcd.txt\n",
      "lfn.txt\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(inpath):\n",
    "    if f == 'README.md' or f.startswith('.'): continue\n",
    "    print(f)\n",
    "    file = open(inpath+f, 'rb') #rb if A, r if NA\n",
    "    chunks = pickle.load(file) # pickle load if A or readlines if NA\n",
    "    file.close()\n",
    "    lang = f[:f.index('.')]\n",
    "    df = pd.DataFrame([chunks, len(chunks)*[lang]], index=['text', 'lang']).T\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rational-insurance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1914517, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(dfs, ignore_index=True)\n",
    "dfs = None #clear memory\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spare-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\x07\\x15\\x04\\x01\\x06\\n\\x07\\x00\\x16\\x12\\x17\\x...</td>\n",
       "      <td>kaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'\\x05\\x0e\\x08\\x01\\x08\\x00\\x1a\\x1f\\x1c7\\t\\x02\\...</td>\n",
       "      <td>kaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b\"\\x01\\x08\\x02\\x03\\x01\\x00\\x08\\x13\\x05\\x0e\\x05...</td>\n",
       "      <td>kaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\\x06\\x02\\x10\\x0b\\x06\\x02\\r\\x00\\x0e\\x05\\x08\\t...</td>\n",
       "      <td>kaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\\x01\\x03\\n\\x02\\x00!\\x04\\x00\\x16\\x02\\x04\\x02\\...</td>\n",
       "      <td>kaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang\n",
       "0  b'\\x07\\x15\\x04\\x01\\x06\\n\\x07\\x00\\x16\\x12\\x17\\x...  kaa\n",
       "1  b'\\x05\\x0e\\x08\\x01\\x08\\x00\\x1a\\x1f\\x1c7\\t\\x02\\...  kaa\n",
       "2  b\"\\x01\\x08\\x02\\x03\\x01\\x00\\x08\\x13\\x05\\x0e\\x05...  kaa\n",
       "3  b'\\x06\\x02\\x10\\x0b\\x06\\x02\\r\\x00\\x0e\\x05\\x08\\t...  kaa\n",
       "4  b'\\x01\\x03\\n\\x02\\x00!\\x04\\x00\\x16\\x02\\x04\\x02\\...  kaa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "together-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1904517</th>\n",
       "      <td>b'\\x0c\\x06\\x05\\x00\\x02\\x03\\t\\x01\\x00\\x08\\x02\\x...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904518</th>\n",
       "      <td>b'\\x00\\x04\\x05\\x0c\\x07\\x0b\\x04\\x00\\x07\\x01\\x00...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904519</th>\n",
       "      <td>b'\\x02\\x0b\\t\\x1c\\x00&amp;\\t\\t\\x06\\x00\\x14\\x04\\x07\\...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904520</th>\n",
       "      <td>b'\\x02\\x00\\x11\\x06\\x05\\t\\x02\\x03\\x00\\x11\\x04\\x...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904521</th>\n",
       "      <td>b'\\x01\\x05\\x04\\x01\\x00\\x04\\x01\\x00\\x06\\x08\\x10...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914512</th>\n",
       "      <td>b'\\x02\\x00\\x02\\x03\\x00\\x0e\\x0b\\x07\\t\\x02\\x00\\r...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914513</th>\n",
       "      <td>b'\\x06\\x00\\r\\x06\\t\\x02\\x00\\x01\\x0f\\x02\\x00\\x07...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914514</th>\n",
       "      <td>b\"\\x01\\x00\\x03\\x0c\\x08\\x04\\x0f\\x02\\x00\\x04\\x03...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914515</th>\n",
       "      <td>b'\\x00\\n\\x02\\x03\\x04\\x05\\x04\\x01\\n\\x01\\x00\\x02...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914516</th>\n",
       "      <td>b'\\x01\\x00\\x03\\r\\x06\\x03\\x01\\x00\\x18\\x01\\t\\x19...</td>\n",
       "      <td>lfn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang\n",
       "1904517  b'\\x0c\\x06\\x05\\x00\\x02\\x03\\t\\x01\\x00\\x08\\x02\\x...  lfn\n",
       "1904518  b'\\x00\\x04\\x05\\x0c\\x07\\x0b\\x04\\x00\\x07\\x01\\x00...  lfn\n",
       "1904519  b'\\x02\\x0b\\t\\x1c\\x00&\\t\\t\\x06\\x00\\x14\\x04\\x07\\...  lfn\n",
       "1904520  b'\\x02\\x00\\x11\\x06\\x05\\t\\x02\\x03\\x00\\x11\\x04\\x...  lfn\n",
       "1904521  b'\\x01\\x05\\x04\\x01\\x00\\x04\\x01\\x00\\x06\\x08\\x10...  lfn\n",
       "...                                                    ...  ...\n",
       "1914512  b'\\x02\\x00\\x02\\x03\\x00\\x0e\\x0b\\x07\\t\\x02\\x00\\r...  lfn\n",
       "1914513  b'\\x06\\x00\\r\\x06\\t\\x02\\x00\\x01\\x0f\\x02\\x00\\x07...  lfn\n",
       "1914514  b\"\\x01\\x00\\x03\\x0c\\x08\\x04\\x0f\\x02\\x00\\x04\\x03...  lfn\n",
       "1914515  b'\\x00\\n\\x02\\x03\\x04\\x05\\x04\\x01\\n\\x01\\x00\\x02...  lfn\n",
       "1914516  b'\\x01\\x00\\x03\\r\\x06\\x03\\x01\\x00\\x18\\x01\\t\\x19...  lfn\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lang']=='lfn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "buried-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(outpath+'chunks_shufanon.pkl', 'wb')\n",
    "pickle.dump(df, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elect-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].map(lambda x: len(x)!=500).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yellow-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpath is file location of input, outpath of output, and anon is bool whether data is\n",
    "# anonymized or not\n",
    "def prepdata(inpath, outfile, anon):\n",
    "    dfs = []\n",
    "    data = pd.DataFrame([], columns=['text', 'lang'])\n",
    "    readmode = 'r'\n",
    "    if anon: readmode = 'rb'\n",
    "    for f in os.listdir(inpath):\n",
    "        if f == 'README.md' or f.startswith('.'): continue\n",
    "#         print(f)\n",
    "        file = open(inpath+f, readmode) #rb if A, r if NA\n",
    "        chunks = [] # pickle load if A or readlines if NA\n",
    "        if anon: chunks = pickle.load(file)\n",
    "        else: chunks = file.readlines()\n",
    "        file.close()\n",
    "        lang = f[:f.index('.')]\n",
    "        df = pd.DataFrame([chunks, len(chunks)*[lang]], index=['text', 'lang']).T\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    dfs = None #clear memory\n",
    "    if not anon: df.text = df.text.apply(lambda x: x[:-1])\n",
    "    f = open(outfile, 'wb')\n",
    "    pickle.dump(df, f)\n",
    "    f.close()\n",
    "    print('anon:', anon, '\\tsanity check:', df.shape[0], 'chunks.', \n",
    "         df['text'].map(lambda x: len(x)!=500).sum(), 'not with 500 chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hourly-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anon: True \tsanity check: 1914517 chunks. 123 not with 500 chars\n"
     ]
    }
   ],
   "source": [
    "inpath = './data/chunks-anon/'\n",
    "outfile= './data/chunks_shufanon.pkl'\n",
    "prepdata(inpath=inpath, outfile=outfile, anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "increased-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anon: False \tsanity check: 1914517 chunks. 123 not with 500 chars\n"
     ]
    }
   ],
   "source": [
    "inpath = './data/chunks-nonanon/'\n",
    "outfile= './data/chunks.pkl'\n",
    "prepdata(inpath, outfile, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
