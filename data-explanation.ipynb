{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explanation\n",
    "In this notebook, the first section explores the dataset and the second section walks through how the dataset was gathered and cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for file operations\n",
    "import pandas as pd\n",
    "import wikipedia # gets list of all languages on wikipedia\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Intro to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'chunked', 'texts', 'extracted']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/snc/Documents/wikidata/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the highest level of my data directory, there are three subdirectories. \n",
    "\n",
    "The extracted subdirectory contains the raw text of all articles for all languages' wikipedias, untouched except for being extracted from the XML format of the Wikipedia server dumps. Each language is its own .txt file in this directory. \n",
    "\n",
    "In texts, I have the data once all the articles's text bodies have been concatenated together into one very long string, with punctuation, URLs, formatting characters and so on stripped away. Each language has its own file(s) in the texts subdirectory - some languages' wikipedias were too big for the language to be held in just one file and subsequently tranferred to another computer (I would get memory errors), so many languages are split into many installments (the number is proportionate to the size of that wikipedia). \n",
    "\n",
    "Lastly, the chunked subdirectory stores a subset of the texts data for more convenient use when trying out machine learning models. Here, each language is represented by its own file. I randomly selected 500-character chunks from each language's files in the texts subdirectory, then shuffled the order of the chunks. For languages with more than 10000 contiguous 500-character chunks, I capped the number of chunks at 10000. However, smaller wikipedias were to small too have all 10000 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The extracted subdirectory\n",
    "Here I'll make a dataframe of all languages/files in extracted and the filesize for that language in megabytes (and recall a gigabyte is 1000 megabytes)\n",
    "### The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>fname</th>\n",
       "      <th>exfsize_MB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kaa</th>\n",
       "      <td>Qaraqalpaqsha</td>\n",
       "      <td>kaa.txt</td>\n",
       "      <td>1.595770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glk</th>\n",
       "      <td>گیلکی</td>\n",
       "      <td>glk.txt</td>\n",
       "      <td>3.559629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>Ελληνικά</td>\n",
       "      <td>el.txt</td>\n",
       "      <td>842.761617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gd</th>\n",
       "      <td>Gàidhlig</td>\n",
       "      <td>gd.txt</td>\n",
       "      <td>8.785653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io</th>\n",
       "      <td>Ido</td>\n",
       "      <td>io.txt</td>\n",
       "      <td>23.850933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lang    fname  exfsize_MB\n",
       "code                                    \n",
       "kaa   Qaraqalpaqsha  kaa.txt    1.595770\n",
       "glk           گیلکی  glk.txt    3.559629\n",
       "el         Ελληνικά   el.txt  842.761617\n",
       "gd         Gàidhlig   gd.txt    8.785653\n",
       "io              Ido   io.txt   23.850933"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_list = []\n",
    "for f in os.listdir(path+'extracted/'):\n",
    "    if f.startswith('.'): continue #skip system files\n",
    "    fname = f[:f.index('.')]\n",
    "    lang = wikipedia.languages()[fname]\n",
    "    extracted_list.append((fname, lang, f, os.path.getsize(path+'extracted/' + f)/1000000))\n",
    "files= pd.DataFrame(extracted_list, columns=['code', 'lang', 'fname', 'exfsize_MB'])\n",
    "files.set_index('code', inplace=True)\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "248 files/languages in the extracted subdirectory - each language in the dataset is represented by its own file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85047.673301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.exfsize_MB.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subdirectory is taking up about 85000 megabytes or 85 gigabytes on my computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "Here is an example file, for Greek (language code el):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path+'extracted/el.txt', 'r')\n",
    "exel = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1473329"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exel) # each title/header and each paragraph of each article is its own line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ελληνικός\\n',\n",
       " 'Σαν επίθετο ελληνικά/ελληνικός σημαίνει από, ή σχετικά με την Ελλάδα, το λαό της, ή την κουλτούρα της.\\n',\n",
       " 'Αθλητισμός\\n',\n",
       " 'Ο αθλητισμός είναι η συστηματική σωματική καλλιέργεια και δράση με συγκεκριμένο τρόπο, ειδική μεθοδολογία και παιδαγωγική με σκοπό την ύψιστη σωματική απόδοση, ως επίδοση σε αθλητικούς αγώνες, στο αθλητικό και κοινωνικό γίγνεσθαι . Παράλληλα ο αθλητισμός είναι ένας κοινωνικός θεσμός ο οποίος αντικατοπτρίζει τη δεδομένη κοινωνία και τον πολιτισμό της. Για παράδειγμα στην Αρχαία Ελλάδα, ο αθλητισμός στην Αθήνα θεωρούταν κοινωνικό και πολιτισμικό αγαθό και είχε παιδαγωγικό χαρακτήρα, ενώ αντίθετα στην Σπάρτη ο αθλητισμός χρησιμοποιούταν για την στρατιωτική εκπαίδευση. Ωστόσο σημαντική είναι η στρωματική διάσταση του αθλητισμού στο πέρασμα του χρόνου. Η γενική τάση ήταν ιδίως τον 18ο και 19ο αιώνα τα κατώτερα κοινωνικά στρώματα να ασχολούνται με τα «λαικα παιχνίδια» όπως το ποδόσφαιρο, ενώ τα ανώτερα κοινωνικά στρώματα με τα «ευγενή αθλήματα» όπως ήταν η ιππασία και η ξιφασκία.\\n',\n",
       " 'Η έννοια του αθλητισμού δεν συνάδει απαραίτητα με την εργασία, όπως αυτή δηλώνεται υπό την στενή έννοια του όρου. Έτσι, υπάρχουν άτομα που ασχολούνται επαγγελματικά ή ημιεπαγγελματικά με τον αθλητισμό, ενώ κάποιοι άλλοι επιδιώκουν τη συμμετοχή τους σε αθλοπαιδιές, προκειμένου απλά να διατηρούνται σε καλή φυσική κατάσταση.\\n',\n",
       " 'Οι επαγγελματίες αθλητές διακρίνονται, συνήθως, για την έντονη επιθυμία τους για διακρίσεις, δυναμισμό, αυστηρή πειθαρχία, αυτοκυριαρχία \\xa0και απόλυτη υπακοή στις υποδείξεις των προπονητών τους σε θέματα που αφορούν στην άσκηση και τη διατροφή τους. Η έντονη σωματική άσκηση και ο ιδιαίτερος τρόπος ζωής που ακολουθεί ένας αθλητής θα πρέπει να συνοδεύεται από αρκετή δόση υπομονής και επιμονής, ώστε να καταφέρει να φθάσει στα επιθυμητά για εκείνον αποτελέσματα. Τα σωματικά προσόντα που απαιτούνται για κάθε άθλημα ποικίλουν. Το ίδιο και οι αμοιβές. Στις περισσότερες των περιπτώσεων, όσο μεγαλύτερη φήμη έχει ο αθλητής ή η ομάδα στην οποία ανήκει, τόσο μεγαλύτερη είναι η αμοιβή που απολαμβάνει (σταθερός μισθός,\\xa0 bonus,\\xa0 χορηγίες).\\n',\n",
       " 'Ωστόσο, πρέπει να διαχωριστεί η έννοια της άθλησης από την έννοια της άσκησης. Η άσκηση γίνεται άθληση όταν αποκτά ανταγωνιστικό χαρακτήρα. Για παράδειγμα ένας που τρέχει στο δρόμο ασκείται, ωστόσο άν έχει κάποιον αντίπαλο ώστε για το ποιος θα τερματίσει πρώτος ή ακόμη και αν ανταγωνίζεται τον ίδιο του τον εαυτό, με το χρονόμετρο, αθλείται. Επίσης πρέπει να προστεθεί και η έννοια, της κίνησης.\\n',\n",
       " 'Ο αθλητισμός μπορεί να πάρει πέντε μορφές, είτε ως ερασιτεχνικός, είτε ως επαγγελματικός, είτε ως μαζικός αθλητισμός, είτε ως φυσικές δραστηριότητες, είτε με την μορφή των παιχνιδιών.\\n',\n",
       " 'Υπάρχουν τρεις θεωρίες σχετικά με τη γένεση του αθλητισμού:\\n',\n",
       " 'Ορισμοί\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exel[0:10] # first 10 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The texts subdirectory\n",
    "### The files\n",
    "Since languages can be split up across multiple files, the naming scheme is as follows when dividing language with code 'lang' into n parts: the first file is 'lang0.txt', second is 'lang1.txt' up until 'lang(n-1).txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-29816dd4895f>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  files['tf_count'][lang] += 1\n",
      "<ipython-input-9-29816dd4895f>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  files['tf_totalsize_MB'][lang] += os.path.getsize(path+'texts/'+f)/1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>fname</th>\n",
       "      <th>exfsize_MB</th>\n",
       "      <th>tf_totalsize_MB</th>\n",
       "      <th>tf_count</th>\n",
       "      <th>tf_avgsize_MB</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kaa</th>\n",
       "      <td>Qaraqalpaqsha</td>\n",
       "      <td>kaa.txt</td>\n",
       "      <td>1.595770</td>\n",
       "      <td>1.452531</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glk</th>\n",
       "      <td>گیلکی</td>\n",
       "      <td>glk.txt</td>\n",
       "      <td>3.559629</td>\n",
       "      <td>3.426984</td>\n",
       "      <td>1</td>\n",
       "      <td>3.426984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>Ελληνικά</td>\n",
       "      <td>el.txt</td>\n",
       "      <td>842.761617</td>\n",
       "      <td>818.755837</td>\n",
       "      <td>4</td>\n",
       "      <td>204.688959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gd</th>\n",
       "      <td>Gàidhlig</td>\n",
       "      <td>gd.txt</td>\n",
       "      <td>8.785653</td>\n",
       "      <td>8.111927</td>\n",
       "      <td>1</td>\n",
       "      <td>8.111927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io</th>\n",
       "      <td>Ido</td>\n",
       "      <td>io.txt</td>\n",
       "      <td>23.850933</td>\n",
       "      <td>20.424057</td>\n",
       "      <td>1</td>\n",
       "      <td>20.424057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lang    fname  exfsize_MB  tf_totalsize_MB  tf_count  \\\n",
       "code                                                                  \n",
       "kaa   Qaraqalpaqsha  kaa.txt    1.595770         1.452531         1   \n",
       "glk           گیلکی  glk.txt    3.559629         3.426984         1   \n",
       "el         Ελληνικά   el.txt  842.761617       818.755837         4   \n",
       "gd         Gàidhlig   gd.txt    8.785653         8.111927         1   \n",
       "io              Ido   io.txt   23.850933        20.424057         1   \n",
       "\n",
       "      tf_avgsize_MB  \n",
       "code                 \n",
       "kaa        1.452531  \n",
       "glk        3.426984  \n",
       "el       204.688959  \n",
       "gd         8.111927  \n",
       "io        20.424057  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['tf_totalsize_MB'] = 0.0 # total size for each language in texts subdirectory\n",
    "files['tf_count'] = 0 # number of files dedicated to that language\n",
    "for f in os.listdir(path + 'texts/'):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')]\n",
    "    lang = ''.join([i for i in lang if not i.isdigit()]) # remove \n",
    "                                                        #installment number to get lang code\n",
    "    files['tf_count'][lang] += 1\n",
    "    files['tf_totalsize_MB'][lang] += os.path.getsize(path+'texts/'+f)/1000000\n",
    "files['tf_avgsize_MB'] = files['tf_totalsize_MB'] / files['tf_count']\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, here is the information for the English dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang               English\n",
       "fname               en.txt\n",
       "exfsize_MB         14186.3\n",
       "tf_totalsize_MB    13384.2\n",
       "tf_count                10\n",
       "tf_avgsize_MB      1338.42\n",
       "Name: en, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.loc['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf_count` entry tells you there's 10 files to store the English wikipedia. The total size of these files (from the `tf_totalsize_MB` entry) is 13384 Megabytes or about 13 gigabytes. The average filesize for each of the 10 English files (since I split the data evenly amongst each file) is 1338 Megabytes or 1.3 gigabytes.\n",
    "\n",
    "Now, you might wonder why I was able to create the 14 gigabyte file to represent English in the extracted directory, but I got a memory error when trying to create one 13 gigabyte file to represent English in the texts directory. I have that question too. Maybe it's because the texts files have no linebreaks while the extracted ones do, making the extracted ones easier to split up when writing to file and transferring to another computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['tf_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80568.077846"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['tf_totalsize_MB'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total texts directory contains 567 files (divided by 248 languages is average 2.3 files per languages). The total space taken up by this subdirectory is about 80.5 gigabytes on my computer, so just slightly smaller than the extracted subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "This is the first file for German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path + 'texts/de0.txt', 'r')\n",
    "tede = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file's contents are just a lot of characters in one continuous line. Other than the very end of the file, the newline character '\\n' is not present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132563807"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n' in tede[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 1000 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Smithee Alan Smithee steht als Pseudonym für einen fiktiven Regisseur der Filme verantwortet bei denen der eigentliche Regisseur seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte Von bis wurde es von der Directors Guild of America DGA für solche Situationen empfohlen seither ist es Thomas Lee Alan Smithee ist jedoch weiterhin in Gebrauch Alternative Schreibweisen sind unter anderem die Ursprungsvariante Allen Smithee sowie Alan Smythee und Adam Smithee Auch zwei teilweise asiatisch anmutende Schreibweisen Alan Smi Thee und Sumishii Aran gehören so die Internet Movie Database dazu Das Pseudonym entstand infolge der Arbeiten am WesternFilm Death of a Gunfighter deutscher Titel Frank Patch Deine Stunden sind gezählt Regisseur Robert Totten und Hauptdarsteller Richard Widmark gerieten in einen Streit woraufhin Don Siegel als neuer Regisseur eingesetzt wurde Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens Handschrift der auch mehr Drehtage als Siegel d'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tede[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chunked subdirectory\n",
    "### The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-c6a859026ee2>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  files['nchunks'][lang] = len(chunks)\n",
      "<ipython-input-17-c6a859026ee2>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  files['fchunk_size_MB'][lang] = os.path.getsize(path+'chunked/'+f)/1000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>fname</th>\n",
       "      <th>exfsize_MB</th>\n",
       "      <th>tf_totalsize_MB</th>\n",
       "      <th>tf_count</th>\n",
       "      <th>tf_avgsize_MB</th>\n",
       "      <th>fchunk_size_MB</th>\n",
       "      <th>nchunks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kaa</th>\n",
       "      <td>Qaraqalpaqsha</td>\n",
       "      <td>kaa.txt</td>\n",
       "      <td>1.595770</td>\n",
       "      <td>1.452531</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452531</td>\n",
       "      <td>1</td>\n",
       "      <td>2746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glk</th>\n",
       "      <td>گیلکی</td>\n",
       "      <td>glk.txt</td>\n",
       "      <td>3.559629</td>\n",
       "      <td>3.426984</td>\n",
       "      <td>1</td>\n",
       "      <td>3.426984</td>\n",
       "      <td>3</td>\n",
       "      <td>3823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>Ελληνικά</td>\n",
       "      <td>el.txt</td>\n",
       "      <td>842.761617</td>\n",
       "      <td>818.755837</td>\n",
       "      <td>4</td>\n",
       "      <td>204.688959</td>\n",
       "      <td>9</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gd</th>\n",
       "      <td>Gàidhlig</td>\n",
       "      <td>gd.txt</td>\n",
       "      <td>8.785653</td>\n",
       "      <td>8.111927</td>\n",
       "      <td>1</td>\n",
       "      <td>8.111927</td>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io</th>\n",
       "      <td>Ido</td>\n",
       "      <td>io.txt</td>\n",
       "      <td>23.850933</td>\n",
       "      <td>20.424057</td>\n",
       "      <td>1</td>\n",
       "      <td>20.424057</td>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lang    fname  exfsize_MB  tf_totalsize_MB  tf_count  \\\n",
       "code                                                                  \n",
       "kaa   Qaraqalpaqsha  kaa.txt    1.595770         1.452531         1   \n",
       "glk           گیلکی  glk.txt    3.559629         3.426984         1   \n",
       "el         Ελληνικά   el.txt  842.761617       818.755837         4   \n",
       "gd         Gàidhlig   gd.txt    8.785653         8.111927         1   \n",
       "io              Ido   io.txt   23.850933        20.424057         1   \n",
       "\n",
       "      tf_avgsize_MB  fchunk_size_MB  nchunks  \n",
       "code                                          \n",
       "kaa        1.452531               1     2746  \n",
       "glk        3.426984               3     3823  \n",
       "el       204.688959               9    10000  \n",
       "gd         8.111927               5    10000  \n",
       "io        20.424057               5    10000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['fchunk_size_MB'] = 0 # file size of chunked file\n",
    "files['nchunks'] = 0 # number of chunks (lines) in the file\n",
    "for f in os.listdir(path+'chunked/'):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')]\n",
    "    file = open(path+'chunked/'+f, 'r')\n",
    "    chunks = file.readlines()\n",
    "    file.close()\n",
    "    files['nchunks'][lang] = len(chunks)\n",
    "    files['fchunk_size_MB'][lang] = os.path.getsize(path+'chunked/'+f)/1000000\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in this histogram of the number of chunks per language, most languages have the full 10000 chunks but a non-negligible amount do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files['nchunks'].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of chunks is 7719 while the median is 10000. The minimum number of chunks is 343."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      248.000000\n",
       "mean      7719.826613\n",
       "std       3258.018658\n",
       "min        343.000000\n",
       "25%       4488.000000\n",
       "50%      10000.000000\n",
       "75%      10000.000000\n",
       "max      10000.000000\n",
       "Name: nchunks, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['nchunks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language with only 343 chunks is Lakku (лакку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>fname</th>\n",
       "      <th>exfsize_MB</th>\n",
       "      <th>tf_totalsize_MB</th>\n",
       "      <th>tf_count</th>\n",
       "      <th>tf_avgsize_MB</th>\n",
       "      <th>fchunk_size_MB</th>\n",
       "      <th>nchunks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lbe</th>\n",
       "      <td>лакку</td>\n",
       "      <td>lbe.txt</td>\n",
       "      <td>0.33415</td>\n",
       "      <td>0.312386</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312386</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lang    fname  exfsize_MB  tf_totalsize_MB  tf_count  tf_avgsize_MB  \\\n",
       "code                                                                         \n",
       "lbe   лакку  lbe.txt     0.33415         0.312386         1       0.312386   \n",
       "\n",
       "      fchunk_size_MB  nchunks  \n",
       "code                           \n",
       "lbe                0      343  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[files['nchunks']==343]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "153 out of the 248 languages (so 62%) have a full 10000 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1366"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['fchunk_size_MB'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files take up 1366 megabytes or 1.4 gigabytes on my computer - much smaller than the extracted or texts subdirectories!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "Here are the Turkish chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path+'chunked/tr.txt', 'r')\n",
    "chtr = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is a chunk. Turkish has 10000 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is 501 characters long (500 characters plus the newline character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(l) for l in chtr])/len(chtr) # average line length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(l)==501 for l in chtr]) # this is the number of lines in the file that have exactly\n",
    "# 501 characters. you see it's equal to the number of lines in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first five chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zı ile karşılaşır Uyku kapsülleri içindeki insanlar etrafa saçılmıştır Bunlardan birisinin rüyalarına giren kadın Julia Rusakova olduğunu görerek şaşırır Bu arada Tetin gönderdiği İHAlar uyku kapsüllerine ateş açar ve Jackin canı pahasına koruduğu kadın dışındaki insanlar ölür Jack kadını Tete haber vermeden kendi küçük kulelerine götürür Vica kadını içeri sokmaz istemez ama Jacke karşı koyamaz Kadın sadece adının Julia olduğunu söyler mekiğin kara kutusunu dinlemeden bir şey söylemeyeceğini söy\\n',\n",
       " 'k dönüşlü konu içeriğini bulabilirsiniz Udo Steinkenin Heinrich Böll Willy Brandt ve HansDietrichGenscher ile sıkı dostluk bağları vardı Onun edebiyattaki katkılarından dolayı hatırasına Steinke Institut Bonn şehrinde kuruldu Enstitüt Udo Steinkenin arşivlerini bünyesinde bulundurmaktadır Ceritler Ceritler soy kökünün Oğuzların Bozok koluna bağlı Beğdili Begtili boyu olduğu bildirilir Yerleştikleri yer olarak da Dulkadiroğlu Beyliğinin sınırları içi olduğu değişik kaynaklarda belirtilmektedir Du\\n',\n",
       " 'pan Gökhan Evliyaoğlu bu dallarda ortaokul yıllarından bu yana ödüller aldı Sanat çalışmalarının fikir ve çalışma hayatının yanı sıra lise döneminde başlayan yazarlık ve gazetecilik mesleğinin her dalında çalışan Evliyaoğlu bütün çalışma hayatı boyunca kültür ve sanat hareketlerinin içinde oldu Akciğer yetmezliği nedeniyle tedavi gören Evliyaoğlu Ağustos gecesi vefat etti Siyaset ve basın alanında önemli izler bırakan Evliyaoğlu için Ağustos tarihinde TBMMde bir tören düzenlendi Evliyaoğlunun ce\\n',\n",
       " ' temsil etmek üzere − y ÷ + mod Ekim günü Esin Eşkinat Esin Eşkinat d Ankara Türk çevirmen Orta öğrenimini TED Ankara Kolejinde tamamladı İstanbul Üniversitesi Edebiyat Fakültesi İngiliz Dili ve Edebiyatından mezun olan Esin Eşkinat İngilizceden şu kitapları Türkçeye kazandırdı ElKut Kut Irakın doğu kesiminde Dicle Nehri kıyısında Vasit ilinin merkezi kent a kadar tüm il bu isimle anılırken bu tarihten sonra Vasıt adını almıştır Bir su dağıtım kanalı olan Şattül Gerafın Dicleden ayrıldığı noktad\\n',\n",
       " 'ye ile Orta Doğu ve Kuzey Afrika arasındaki ticaret işlemlerinde uzmanlaşan ilk Türk bankasıdır Kuruluş amacı gelecek vadeden bu bölgede ticareti artırmak için taraflar arasında köprü görevi görmek ve ticari ilişkileri desteklemektir Aktiflerinin değeri milyar doları aşan Libya Merkez Bankasının sahibi olduğu The Libyan Foreign Bank AT BANKın en büyük ortağıdır Bununla birlikte onu Türk bankacılık sektörünün aktif büyüklük bakımından yaklaşık unu ellerinde bulunduran Türkiyenin en büyük iki bank\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chtr[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Explanation of data gathering pipeline\n",
    "Now that the dataset is introduced, I'll explain how I gathered it. To assemble this full, final copy of the dataset, I ran my datagathering script on CRC over a period of days. The script is located in the datagather/crc directory of this repository.\n",
    "\n",
    "I'm not sure how long the script itself would take on its own now because as I ran it, it would crash for various reasons (eg, writing large strings causing memory errors or networking set up wrong) or my session would expire, so I would need to fix the issue and start it running again where it left off. Sometimes I also had up to six copies of the script running at a time over the weekend when CRC had a lot of open resources, each collecting languages starting with a different letter of the alphabet.\n",
    "## 1: Networking setup\n",
    "CRC doesn't have enough storage space for the full dataset, so I needed to periodically send data somewhere else and delete all of one language's files from CRC before moving onto the next language. I would send files to an old laptop I have for storage (see second progress report for more details). \n",
    "\n",
    "Here is an outline of the networking aspects involved in my data gathering/cleaning:\n",
    "\n",
    "Outside script\n",
    "\n",
    "1. Get an internet address (or technically, publically expose port 22) for the storage laptop by running `./ngrok tcp 22` in shell and leaving it running\n",
    "2. On my normal laptop, connect to Pitt's VPN (required to access CRC)\n",
    "2. ssh to crc.h2p.pitt.edu to access CRC\n",
    "3. Pass storage laptop's username, address and port to the script as command line arguments\n",
    "\n",
    "Inside script\n",
    "\n",
    "1. Prompt user for password, store it for future use in the runtime\n",
    "2. Establish a test sftp connection with storage laptop to verify password (if fails, prompt user again)\n",
    "3. After downloading a language's raw dump and extracting XML, send backup to device by opening a new sftp connection, transferring file, then closing connection\n",
    "4. Transfer another copy of the data after passing through the cleaning steps\n",
    "5. Transfer another copy of the data after the chunking step\n",
    "\n",
    "What follows is an example that creates a very small exammple helloworld.txt file, then transfers it to another computer. I didn't bother with the VPN or ngrok for this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test file\n",
    "fname = 'helloworld.txt'\n",
    "f = open(fname, 'w')\n",
    "f.writelines('hello world\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'snc'\n",
    "port = 22\n",
    "address = '10.0.0.25' # this is the computer's PRIVATE IP address: the address your internet\n",
    "# router uses to distinguish this computer from any other device on your own network so \n",
    "# you don't recieve all the webpages requested by your brother and grandma or vice versa\n",
    "# you can entering the IP address for your own computer here and use your laptop as the source \n",
    "# and the destination if you want to try it out. Then, it would be a bit like emailing or calling\n",
    "# yourself in that the email is sent to the sender or the call is sent to the caller.\n",
    "\n",
    "# to enable this sftp connection for a mac, go to System Preferences > Sharing > \n",
    "# Enable \"Remote Login\". There, it will display the IP address to use. Your source/local\n",
    "# and destination/remote computers must be on the same wifi network unless you learn to use\n",
    "# ngrok or something similar. Don't know how to do it on other operating systems \n",
    "# but there's probably tutorials on line if it isn't enabled by defailt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sftp password: ········\n"
     ]
    }
   ],
   "source": [
    "pwd = getpass.getpass(prompt='sftp password: ') # password for your username \n",
    "# on destination/remote computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttransferred helloworld.txt\n"
     ]
    }
   ],
   "source": [
    "# ESTABLISH CONNECTION\n",
    "client = paramiko.client.SSHClient()\n",
    "client.load_system_host_keys() # this loads any local ssh keys\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "client.connect(address, port=port, username=username, password=pwd)\n",
    "sftp = client.open_sftp() # type SFTPClient\n",
    "\n",
    "# TRANSFER THE FILE\n",
    "sftp.put('./'+fname, fname) #src, dest path/filename\n",
    "print('transferred', fname)\n",
    "\n",
    "# CLOSE THE CONNECTION\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this yourself, look in your home (~) directory for the transferred file at the destination/remote computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Find the data files to download\n",
    "The Wikipedia API offers a dictionary of all the languages of a Wikipedia edition that exists or used to exist. It provides both the language's abbreviation/code and name. So, my code iterates through this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 'Qafár af',\n",
       " 'ab': 'Аҧсшәа',\n",
       " 'abs': 'bahasa ambon',\n",
       " 'ace': 'Acèh',\n",
       " 'ady': 'адыгабзэ',\n",
       " 'ady-cyrl': 'адыгабзэ',\n",
       " 'aeb': 'تونسي/Tûnsî',\n",
       " 'aeb-arab': 'تونسي',\n",
       " 'aeb-latn': 'Tûnsî',\n",
       " 'af': 'Afrikaans',\n",
       " 'ak': 'Akan',\n",
       " 'aln': 'Gegë',\n",
       " 'als': 'Alemannisch',\n",
       " 'alt': 'алтай тил',\n",
       " 'am': 'አማርኛ',\n",
       " 'ami': 'Pangcah',\n",
       " 'an': 'aragonés',\n",
       " 'ang': 'Ænglisc',\n",
       " 'anp': 'अङ्गिका',\n",
       " 'ar': 'العربية',\n",
       " 'arc': 'ܐܪܡܝܐ',\n",
       " 'arn': 'mapudungun',\n",
       " 'arq': 'جازايرية',\n",
       " 'ary': 'الدارجة',\n",
       " 'arz': 'مصرى',\n",
       " 'as': 'অসমীয়া',\n",
       " 'ase': 'American sign language',\n",
       " 'ast': 'asturianu',\n",
       " 'atj': 'Atikamekw',\n",
       " 'av': 'авар',\n",
       " 'avk': 'Kotava',\n",
       " 'awa': 'अवधी',\n",
       " 'ay': 'Aymar aru',\n",
       " 'az': 'azərbaycanca',\n",
       " 'azb': 'تۆرکجه',\n",
       " 'ba': 'башҡортса',\n",
       " 'ban': 'Basa Bali',\n",
       " 'ban-bali': 'ᬩᬲᬩᬮᬶ',\n",
       " 'bar': 'Boarisch',\n",
       " 'bat-smg': 'žemaitėška',\n",
       " 'bbc': 'Batak Toba',\n",
       " 'bbc-latn': 'Batak Toba',\n",
       " 'bcc': 'جهلسری بلوچی',\n",
       " 'bcl': 'Bikol Central',\n",
       " 'be': 'беларуская',\n",
       " 'be-tarask': 'беларуская (тарашкевіца)\\u200e',\n",
       " 'be-x-old': 'беларуская (тарашкевіца)\\u200e',\n",
       " 'bg': 'български',\n",
       " 'bgn': 'روچ کپتین بلوچی',\n",
       " 'bh': 'भोजपुरी',\n",
       " 'bho': 'भोजपुरी',\n",
       " 'bi': 'Bislama',\n",
       " 'bjn': 'Banjar',\n",
       " 'bm': 'bamanankan',\n",
       " 'bn': 'বাংলা',\n",
       " 'bo': 'བོད་ཡིག',\n",
       " 'bpy': 'বিষ্ণুপ্রিয়া মণিপুরী',\n",
       " 'bqi': 'بختیاری',\n",
       " 'br': 'brezhoneg',\n",
       " 'brh': 'Bráhuí',\n",
       " 'bs': 'bosanski',\n",
       " 'btm': 'Batak Mandailing',\n",
       " 'bto': 'Iriga Bicolano',\n",
       " 'bug': 'ᨅᨔ ᨕᨘᨁᨗ',\n",
       " 'bxr': 'буряад',\n",
       " 'ca': 'català',\n",
       " 'cbk-zam': 'Chavacano de Zamboanga',\n",
       " 'cdo': 'Mìng-dĕ̤ng-ngṳ̄',\n",
       " 'ce': 'нохчийн',\n",
       " 'ceb': 'Cebuano',\n",
       " 'ch': 'Chamoru',\n",
       " 'cho': 'Choctaw',\n",
       " 'chr': 'ᏣᎳᎩ',\n",
       " 'chy': 'Tsetsêhestâhese',\n",
       " 'ckb': 'کوردی',\n",
       " 'co': 'corsu',\n",
       " 'cps': 'Capiceño',\n",
       " 'cr': 'Nēhiyawēwin / ᓀᐦᐃᔭᐍᐏᐣ',\n",
       " 'crh': 'qırımtatarca',\n",
       " 'crh-cyrl': 'къырымтатарджа (Кирилл)\\u200e',\n",
       " 'crh-latn': 'qırımtatarca (Latin)\\u200e',\n",
       " 'cs': 'čeština',\n",
       " 'csb': 'kaszëbsczi',\n",
       " 'cu': 'словѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟ',\n",
       " 'cv': 'Чӑвашла',\n",
       " 'cy': 'Cymraeg',\n",
       " 'da': 'dansk',\n",
       " 'de': 'Deutsch',\n",
       " 'de-at': 'Österreichisches Deutsch',\n",
       " 'de-ch': 'Schweizer Hochdeutsch',\n",
       " 'de-formal': 'Deutsch (Sie-Form)\\u200e',\n",
       " 'din': 'Thuɔŋjäŋ',\n",
       " 'diq': 'Zazaki',\n",
       " 'dsb': 'dolnoserbski',\n",
       " 'dtp': 'Dusun Bundu-liwan',\n",
       " 'dty': 'डोटेली',\n",
       " 'dv': 'ދިވެހިބަސް',\n",
       " 'dz': 'ཇོང་ཁ',\n",
       " 'ee': 'eʋegbe',\n",
       " 'egl': 'Emiliàn',\n",
       " 'el': 'Ελληνικά',\n",
       " 'eml': 'emiliàn e rumagnòl',\n",
       " 'en': 'English',\n",
       " 'en-ca': 'Canadian English',\n",
       " 'en-gb': 'British English',\n",
       " 'eo': 'Esperanto',\n",
       " 'es': 'español',\n",
       " 'es-419': 'español de América Latina',\n",
       " 'es-formal': 'español (formal)\\u200e',\n",
       " 'et': 'eesti',\n",
       " 'eu': 'euskara',\n",
       " 'ext': 'estremeñu',\n",
       " 'fa': 'فارسی',\n",
       " 'ff': 'Fulfulde',\n",
       " 'fi': 'suomi',\n",
       " 'fit': 'meänkieli',\n",
       " 'fiu-vro': 'võro',\n",
       " 'fj': 'Na Vosa Vakaviti',\n",
       " 'fo': 'føroyskt',\n",
       " 'fr': 'français',\n",
       " 'frc': 'français cadien',\n",
       " 'frp': 'arpetan',\n",
       " 'frr': 'Nordfriisk',\n",
       " 'fur': 'furlan',\n",
       " 'fy': 'Frysk',\n",
       " 'ga': 'Gaeilge',\n",
       " 'gag': 'Gagauz',\n",
       " 'gan': '贛語',\n",
       " 'gan-hans': '赣语（简体）\\u200e',\n",
       " 'gan-hant': '贛語（繁體）\\u200e',\n",
       " 'gcr': 'kriyòl gwiyannen',\n",
       " 'gd': 'Gàidhlig',\n",
       " 'gl': 'galego',\n",
       " 'glk': 'گیلکی',\n",
       " 'gn': \"Avañe'ẽ\",\n",
       " 'gom': 'गोंयची कोंकणी / Gõychi Konknni',\n",
       " 'gom-deva': 'गोंयची कोंकणी',\n",
       " 'gom-latn': 'Gõychi Konknni',\n",
       " 'gor': 'Bahasa Hulontalo',\n",
       " 'got': '𐌲𐌿𐍄𐌹𐍃𐌺',\n",
       " 'grc': 'Ἀρχαία ἑλληνικὴ',\n",
       " 'gsw': 'Alemannisch',\n",
       " 'gu': 'ગુજરાતી',\n",
       " 'guc': 'wayuunaiki',\n",
       " 'gv': 'Gaelg',\n",
       " 'ha': 'Hausa',\n",
       " 'hak': '客家語/Hak-kâ-ngî',\n",
       " 'haw': 'Hawaiʻi',\n",
       " 'he': 'עברית',\n",
       " 'hi': 'हिन्दी',\n",
       " 'hif': 'Fiji Hindi',\n",
       " 'hif-latn': 'Fiji Hindi',\n",
       " 'hil': 'Ilonggo',\n",
       " 'ho': 'Hiri Motu',\n",
       " 'hr': 'hrvatski',\n",
       " 'hrx': 'Hunsrik',\n",
       " 'hsb': 'hornjoserbsce',\n",
       " 'ht': 'Kreyòl ayisyen',\n",
       " 'hu': 'magyar',\n",
       " 'hu-formal': 'magyar (formal)\\u200e',\n",
       " 'hy': 'հայերեն',\n",
       " 'hyw': 'Արեւմտահայերէն',\n",
       " 'hz': 'Otsiherero',\n",
       " 'ia': 'interlingua',\n",
       " 'id': 'Bahasa Indonesia',\n",
       " 'ie': 'Interlingue',\n",
       " 'ig': 'Igbo',\n",
       " 'ii': 'ꆇꉙ',\n",
       " 'ik': 'Iñupiak',\n",
       " 'ike-cans': 'ᐃᓄᒃᑎᑐᑦ',\n",
       " 'ike-latn': 'inuktitut',\n",
       " 'ilo': 'Ilokano',\n",
       " 'inh': 'ГӀалгӀай',\n",
       " 'io': 'Ido',\n",
       " 'is': 'íslenska',\n",
       " 'it': 'italiano',\n",
       " 'iu': 'ᐃᓄᒃᑎᑐᑦ/inuktitut',\n",
       " 'ja': '日本語',\n",
       " 'jam': 'Patois',\n",
       " 'jbo': 'la .lojban.',\n",
       " 'jut': 'jysk',\n",
       " 'jv': 'Jawa',\n",
       " 'ka': 'ქართული',\n",
       " 'kaa': 'Qaraqalpaqsha',\n",
       " 'kab': 'Taqbaylit',\n",
       " 'kbd': 'Адыгэбзэ',\n",
       " 'kbd-cyrl': 'Адыгэбзэ',\n",
       " 'kbp': 'Kabɩyɛ',\n",
       " 'kcg': 'Tyap',\n",
       " 'kg': 'Kongo',\n",
       " 'khw': 'کھوار',\n",
       " 'ki': 'Gĩkũyũ',\n",
       " 'kiu': 'Kırmancki',\n",
       " 'kj': 'Kwanyama',\n",
       " 'kjp': 'ဖၠုံလိက်',\n",
       " 'kk': 'қазақша',\n",
       " 'kk-arab': 'قازاقشا (تٴوتە)\\u200f',\n",
       " 'kk-cn': 'قازاقشا (جۇنگو)\\u200f',\n",
       " 'kk-cyrl': 'қазақша (кирил)\\u200e',\n",
       " 'kk-kz': 'қазақша (Қазақстан)\\u200e',\n",
       " 'kk-latn': 'qazaqşa (latın)\\u200e',\n",
       " 'kk-tr': 'qazaqşa (Türkïya)\\u200e',\n",
       " 'kl': 'kalaallisut',\n",
       " 'km': 'ភាសាខ្មែរ',\n",
       " 'kn': 'ಕನ್ನಡ',\n",
       " 'ko': '한국어',\n",
       " 'ko-kp': '조선말',\n",
       " 'koi': 'перем коми',\n",
       " 'kr': 'Kanuri',\n",
       " 'krc': 'къарачай-малкъар',\n",
       " 'kri': 'Krio',\n",
       " 'krj': 'Kinaray-a',\n",
       " 'krl': 'karjal',\n",
       " 'ks': 'कॉशुर / کٲشُر',\n",
       " 'ks-arab': 'کٲشُر',\n",
       " 'ks-deva': 'कॉशुर',\n",
       " 'ksh': 'Ripoarisch',\n",
       " 'ku': 'kurdî',\n",
       " 'ku-arab': 'كوردي (عەرەبی)\\u200f',\n",
       " 'ku-latn': 'kurdî (latînî)\\u200e',\n",
       " 'kum': 'къумукъ',\n",
       " 'kv': 'коми',\n",
       " 'kw': 'kernowek',\n",
       " 'ky': 'Кыргызча',\n",
       " 'la': 'Latina',\n",
       " 'lad': 'Ladino',\n",
       " 'lb': 'Lëtzebuergesch',\n",
       " 'lbe': 'лакку',\n",
       " 'lez': 'лезги',\n",
       " 'lfn': 'Lingua Franca Nova',\n",
       " 'lg': 'Luganda',\n",
       " 'li': 'Limburgs',\n",
       " 'lij': 'Ligure',\n",
       " 'liv': 'Līvõ kēļ',\n",
       " 'lki': 'لەکی',\n",
       " 'lld': 'Ladin',\n",
       " 'lmo': 'lumbaart',\n",
       " 'ln': 'lingála',\n",
       " 'lo': 'ລາວ',\n",
       " 'loz': 'Silozi',\n",
       " 'lrc': 'لۊری شومالی',\n",
       " 'lt': 'lietuvių',\n",
       " 'ltg': 'latgaļu',\n",
       " 'lus': 'Mizo ţawng',\n",
       " 'luz': 'لئری دوٙمینی',\n",
       " 'lv': 'latviešu',\n",
       " 'lzh': '文言',\n",
       " 'lzz': 'Lazuri',\n",
       " 'mad': 'Madhurâ',\n",
       " 'mai': 'मैथिली',\n",
       " 'map-bms': 'Basa Banyumasan',\n",
       " 'mdf': 'мокшень',\n",
       " 'mg': 'Malagasy',\n",
       " 'mh': 'Ebon',\n",
       " 'mhr': 'олык марий',\n",
       " 'mi': 'Māori',\n",
       " 'min': 'Minangkabau',\n",
       " 'mk': 'македонски',\n",
       " 'ml': 'മലയാളം',\n",
       " 'mn': 'монгол',\n",
       " 'mni': 'ꯃꯤꯇꯩ ꯂꯣꯟ',\n",
       " 'mnw': 'ဘာသာ မန်',\n",
       " 'mo': 'молдовеняскэ',\n",
       " 'mr': 'मराठी',\n",
       " 'mrh': 'Mara',\n",
       " 'mrj': 'кырык мары',\n",
       " 'ms': 'Bahasa Melayu',\n",
       " 'mt': 'Malti',\n",
       " 'mus': 'Mvskoke',\n",
       " 'mwl': 'Mirandés',\n",
       " 'my': 'မြန်မာဘာသာ',\n",
       " 'myv': 'эрзянь',\n",
       " 'mzn': 'مازِرونی',\n",
       " 'na': 'Dorerin Naoero',\n",
       " 'nah': 'Nāhuatl',\n",
       " 'nan': 'Bân-lâm-gú',\n",
       " 'nap': 'Napulitano',\n",
       " 'nb': 'norsk bokmål',\n",
       " 'nds': 'Plattdüütsch',\n",
       " 'nds-nl': 'Nedersaksies',\n",
       " 'ne': 'नेपाली',\n",
       " 'new': 'नेपाल भाषा',\n",
       " 'ng': 'Oshiwambo',\n",
       " 'nia': 'Li Niha',\n",
       " 'niu': 'Niuē',\n",
       " 'nl': 'Nederlands',\n",
       " 'nl-informal': 'Nederlands (informeel)\\u200e',\n",
       " 'nn': 'norsk nynorsk',\n",
       " 'no': 'norsk',\n",
       " 'nov': 'Novial',\n",
       " 'nqo': 'ߒߞߏ',\n",
       " 'nrm': 'Nouormand',\n",
       " 'nso': 'Sesotho sa Leboa',\n",
       " 'nv': 'Diné bizaad',\n",
       " 'ny': 'Chi-Chewa',\n",
       " 'nys': 'Nyunga',\n",
       " 'oc': 'occitan',\n",
       " 'olo': 'livvinkarjala',\n",
       " 'om': 'Oromoo',\n",
       " 'or': 'ଓଡ଼ିଆ',\n",
       " 'os': 'Ирон',\n",
       " 'pa': 'ਪੰਜਾਬੀ',\n",
       " 'pag': 'Pangasinan',\n",
       " 'pam': 'Kapampangan',\n",
       " 'pap': 'Papiamentu',\n",
       " 'pcd': 'Picard',\n",
       " 'pdc': 'Deitsch',\n",
       " 'pdt': 'Plautdietsch',\n",
       " 'pfl': 'Pälzisch',\n",
       " 'pi': 'पालि',\n",
       " 'pih': 'Norfuk / Pitkern',\n",
       " 'pl': 'polski',\n",
       " 'pms': 'Piemontèis',\n",
       " 'pnb': 'پنجابی',\n",
       " 'pnt': 'Ποντιακά',\n",
       " 'prg': 'Prūsiskan',\n",
       " 'ps': 'پښتو',\n",
       " 'pt': 'português',\n",
       " 'pt-br': 'português do Brasil',\n",
       " 'qu': 'Runa Simi',\n",
       " 'qug': 'Runa shimi',\n",
       " 'rgn': 'Rumagnôl',\n",
       " 'rif': 'Tarifit',\n",
       " 'rm': 'rumantsch',\n",
       " 'rmy': 'romani čhib',\n",
       " 'rn': 'Kirundi',\n",
       " 'ro': 'română',\n",
       " 'roa-rup': 'armãneashti',\n",
       " 'roa-tara': 'tarandíne',\n",
       " 'ru': 'русский',\n",
       " 'rue': 'русиньскый',\n",
       " 'rup': 'armãneashti',\n",
       " 'ruq': 'Vlăheşte',\n",
       " 'ruq-cyrl': 'Влахесте',\n",
       " 'ruq-latn': 'Vlăheşte',\n",
       " 'rw': 'Kinyarwanda',\n",
       " 'sa': 'संस्कृतम्',\n",
       " 'sah': 'саха тыла',\n",
       " 'sat': 'ᱥᱟᱱᱛᱟᱲᱤ',\n",
       " 'sc': 'sardu',\n",
       " 'scn': 'sicilianu',\n",
       " 'sco': 'Scots',\n",
       " 'sd': 'سنڌي',\n",
       " 'sdc': 'Sassaresu',\n",
       " 'sdh': 'کوردی خوارگ',\n",
       " 'se': 'davvisámegiella',\n",
       " 'sei': 'Cmique Itom',\n",
       " 'ses': 'Koyraboro Senni',\n",
       " 'sg': 'Sängö',\n",
       " 'sgs': 'žemaitėška',\n",
       " 'sh': 'srpskohrvatski / српскохрватски',\n",
       " 'shi': 'Taclḥit',\n",
       " 'shi-latn': 'Taclḥit',\n",
       " 'shi-tfng': 'ⵜⴰⵛⵍⵃⵉⵜ',\n",
       " 'shn': 'ၽႃႇသႃႇတႆး ',\n",
       " 'shy': 'tacawit',\n",
       " 'shy-latn': 'tacawit',\n",
       " 'si': 'සිංහල',\n",
       " 'simple': 'Simple English',\n",
       " 'sk': 'slovenčina',\n",
       " 'skr': 'سرائیکی',\n",
       " 'skr-arab': 'سرائیکی',\n",
       " 'sl': 'slovenščina',\n",
       " 'sli': 'Schläsch',\n",
       " 'sm': 'Gagana Samoa',\n",
       " 'sma': 'åarjelsaemien',\n",
       " 'smn': 'anarâškielâ',\n",
       " 'sn': 'chiShona',\n",
       " 'so': 'Soomaaliga',\n",
       " 'sq': 'shqip',\n",
       " 'sr': 'српски / srpski',\n",
       " 'sr-ec': 'српски (ћирилица)\\u200e',\n",
       " 'sr-el': 'srpski (latinica)\\u200e',\n",
       " 'srn': 'Sranantongo',\n",
       " 'ss': 'SiSwati',\n",
       " 'st': 'Sesotho',\n",
       " 'stq': 'Seeltersk',\n",
       " 'sty': 'себертатар',\n",
       " 'su': 'Sunda',\n",
       " 'sv': 'svenska',\n",
       " 'sw': 'Kiswahili',\n",
       " 'szl': 'ślůnski',\n",
       " 'szy': 'Sakizaya',\n",
       " 'ta': 'தமிழ்',\n",
       " 'tay': 'Tayal',\n",
       " 'tcy': 'ತುಳು',\n",
       " 'te': 'తెలుగు',\n",
       " 'tet': 'tetun',\n",
       " 'tg': 'тоҷикӣ',\n",
       " 'tg-cyrl': 'тоҷикӣ',\n",
       " 'tg-latn': 'tojikī',\n",
       " 'th': 'ไทย',\n",
       " 'ti': 'ትግርኛ',\n",
       " 'tk': 'Türkmençe',\n",
       " 'tl': 'Tagalog',\n",
       " 'tly': 'tolışi',\n",
       " 'tly-cyrl': 'толыши',\n",
       " 'tn': 'Setswana',\n",
       " 'to': 'lea faka-Tonga',\n",
       " 'tpi': 'Tok Pisin',\n",
       " 'tr': 'Türkçe',\n",
       " 'tru': 'Ṫuroyo',\n",
       " 'trv': 'Seediq',\n",
       " 'ts': 'Xitsonga',\n",
       " 'tt': 'татарча/tatarça',\n",
       " 'tt-cyrl': 'татарча',\n",
       " 'tt-latn': 'tatarça',\n",
       " 'tum': 'chiTumbuka',\n",
       " 'tw': 'Twi',\n",
       " 'ty': 'reo tahiti',\n",
       " 'tyv': 'тыва дыл',\n",
       " 'tzm': 'ⵜⴰⵎⴰⵣⵉⵖⵜ',\n",
       " 'udm': 'удмурт',\n",
       " 'ug': 'ئۇيغۇرچە / Uyghurche',\n",
       " 'ug-arab': 'ئۇيغۇرچە',\n",
       " 'ug-latn': 'Uyghurche',\n",
       " 'uk': 'українська',\n",
       " 'ur': 'اردو',\n",
       " 'uz': 'oʻzbekcha/ўзбекча',\n",
       " 'uz-cyrl': 'ўзбекча',\n",
       " 'uz-latn': 'oʻzbekcha',\n",
       " 've': 'Tshivenda',\n",
       " 'vec': 'vèneto',\n",
       " 'vep': 'vepsän kel’',\n",
       " 'vi': 'Tiếng Việt',\n",
       " 'vls': 'West-Vlams',\n",
       " 'vmf': 'Mainfränkisch',\n",
       " 'vo': 'Volapük',\n",
       " 'vot': 'Vaďďa',\n",
       " 'vro': 'võro',\n",
       " 'wa': 'walon',\n",
       " 'war': 'Winaray',\n",
       " 'wo': 'Wolof',\n",
       " 'wuu': '吴语',\n",
       " 'xal': 'хальмг',\n",
       " 'xh': 'isiXhosa',\n",
       " 'xmf': 'მარგალური',\n",
       " 'xsy': 'saisiyat',\n",
       " 'yi': 'ייִדיש',\n",
       " 'yo': 'Yorùbá',\n",
       " 'yue': '粵語',\n",
       " 'za': 'Vahcuengh',\n",
       " 'zea': 'Zeêuws',\n",
       " 'zgh': 'ⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ',\n",
       " 'zh': '中文',\n",
       " 'zh-classical': '文言',\n",
       " 'zh-cn': '中文（中国大陆）\\u200e',\n",
       " 'zh-hans': '中文（简体）\\u200e',\n",
       " 'zh-hant': '中文（繁體）\\u200e',\n",
       " 'zh-hk': '中文（香港）\\u200e',\n",
       " 'zh-min-nan': 'Bân-lâm-gú',\n",
       " 'zh-mo': '中文（澳門）\\u200e',\n",
       " 'zh-my': '中文（马来西亚）\\u200e',\n",
       " 'zh-sg': '中文（新加坡）\\u200e',\n",
       " 'zh-tw': '中文（台灣）\\u200e',\n",
       " 'zh-yue': '粵語',\n",
       " 'zu': 'isiZulu'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langsdict = wikipedia.languages()\n",
    "langsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I have the language's abbreviation `abv`, I can construct the download link for the wikipedia dump as: \n",
    "\n",
    "'*https://dumps.wikimedia.org/*' + `abv` + '*wiki/latest/*' + `abv` + '*wiki-latest-pages-articles.xml.bz2*'\n",
    "\n",
    "Once I download it, I store it on my computer. Then, I check to see if the file is already present locally before re-downloading. This way I don't need to keep stressing Wikipedia's server each time I run my program. Some wikipedias are closed/no longer maintained and throw an error if I try to download, so I just skip those languages. Also, I deem languages as too small if their raw dump file size is less than 1MB, I'll discuss more why later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Download raw wikipedia XML files\n",
    "The script downloads these automatically, they come in a compressed .xml.bz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datagather/dumps/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-41a150578dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdumpspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./datagather/dumps/'\u001b[0m \u001b[0;31m# where I'm storing the raw files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdumpsraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumpspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# isolate just name of the language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datagather/dumps/'"
     ]
    }
   ],
   "source": [
    "dumpspath = './datagather/dumps/' # where I'm storing the raw files\n",
    "dumpsraw = dict()\n",
    "for f in os.listdir(dumpspath):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')-4] # isolate just name of the language\n",
    "    dumpsraw[lang] = [langsdict[lang], os.path.getsize(dumpspath + f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe of the languages I've gotten - 248 languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = pandas.DataFrame.from_dict(dumpsraw, columns=['name', 'fsize'], orient='index')\n",
    "print(dumps.shape)\n",
    "dumps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of bytes for these raw dump files alone, which is about 76 gigabytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps['fsize'].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the languages I skipped for either of the reasons I mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = [langsdict[l] for l in langsdict.keys() if l not in dumps.index]\n",
    "print(len(excluded), 'languages excluded:\\n', ', '.join(excluded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But these are the ones I was able to get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dumps.shape[0], 'languages included:\\n', ', '.join(dumps['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest wikipedias by raw dump file size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps.sort_values('fsize', ascending=False).head(10) # largest wikis included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English alone is about 18 GB! The distant second is German at about 6 GB.\n",
    "\n",
    "Smallest file sizes that I included (since cutoff was 1MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps.sort_values('fsize').head(10) # smallest wikis included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the filesizes for the languages I included. You can see most are quite small, while a few are very big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps['fsize'].plot.hist(bins=30) # many small wikis, just a few big wikis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of Qaraqalpaqsha's raw dump file, if I manually decompress it (I don't store these anywhere, it's all handled by the extractor tool I use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-raw.xml', 'r')\n",
    "exdump = f.readlines()\n",
    "f.close()\n",
    "exdump[:20] # start of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exdump[3745:3765] # random middle part showing part of an article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the languages that ultimately gets excluded is Qafár af. Here I'll read in its expanded dump file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/aa-raw.xml')\n",
    "aaraw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaraw[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aaraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this language's XML file has 5526 lines, which sounds like enough to at least get a good sampling of what the language looks like. But then I stripped all the XML and extra Wikipedia information off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Remove the XML and wikipedia formatting\n",
    "Wikipedia formatting is stuff like the particular templates used in the articles, the random stuff you see at the very start of the above raw XML file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/aa-extracted.txt')\n",
    "aa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is Qafár af (the example excluded language)'s *entire* file without the XML and extra Wikipedia formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output of the tool I used is one json object per page/article. From 5526 lines, Qafár af is left with just two pages, one of which has no text at all. The other page is just a sentence long; it seems to be a message notifying users that this wikipedia is closed and not maintained. This is why I excluded languages whose raw dump files are smaller than 1MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to Qaraqalpaqsha, which I showed the raw dump for earlier, here is its Wikipedia with the XML and Wikipedia formatting removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-extracted.txt', 'r')\n",
    "kaa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaa[:3] # first 3 json objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding is causing some weird display issues for some characters here, it's strange because it shows up fine in my text editor and in zsh with the `head` command. Either way, it gets corrected once it travels through the rest of my current data correction pipeline and the final file for this language which I'll show later displays fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool I'm using to go from raw dump to \"clean\" json items is called WikiExtractor. The current version on GitHub is not stable, but it's here: https://github.com/attardi/wikiextractor\n",
    "\n",
    "I forked it and made a stable version here: https://github.com/soCromp/wikiextractor\n",
    "\n",
    "I needed to use specifically this tool for another project I'm doing in another class and spent a lot of time fixing it and figuring out how to run it. It's designed specifically for processing Wikipedia data en masse and already \"knows\" Wikipedia formatting and how to find certain article attributes. As a result, I decided to just use it again here rather than learn how to use totally another thing like Beautiful Soup - I've used their Java library a couple years ago (jsoup) and remembered taking a while to get data cleaned like I wanted. \n",
    "\n",
    "The tool runs pretty quick - Afrikaans takes 1-2 minutes and English maybe 5. I also pass the text through some shell regex like `sed` at this point for a little extra cleaning. Shell regex seems to run faster than Python regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Get the article text\n",
    "The next step in the pipeline is doing some encoding correction after the XML and Wikipedia formatting is extracted. Then, I grab just article text from the json objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-articles.txt')\n",
    "aftexts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(aftexts[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it still has line breaks, punctuation, numbers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, I was planning to tokenize. I added this in and started to run the script on all the data, but disliked the results I was getting. I don't know what tool I could use to tokenize different languages since there are so many different rules and probably a lot of things I don't know about how to tokenize the languages I don't speak. So, I just took the tokenizing step out at least for now.\n",
    "## 6: Cleaning to remove punctuation, extra spaces\n",
    "Next, I just pass the text through a filter to remove all puntuation characters and replace sequences of multiple spaces in a row with just one space. I use a special unicode library function, unicodedata.category() to detect any unicode puntuation character in order to catch even puntuation like the Japanese period \"。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-text.txt')\n",
    "afclean = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afclean[0][:402]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is some strange encoding issue when I display it in Jupyter. You can see it in the spaces between where the numbers were. It doesn't occur when I display it in Atom, Sublime or TextEdit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Chunking and shuffling\n",
    "Originally I had the idea to make each line a sentence or a certain number of words. Then I realized not all languages separate words or sentences the same way, and decided the most language-neutral way to do this would be to simply make each line 500 characters long. I refer to each line as a chunk, and simply divide the long block of text every 500 characters. I'm sure there are better ways to handle this, but I chose this simple way for now at least. Then, if there are more than 100000 chunks, I randomly sample 100000 chunks to prevent any larger languages from being overrepresented in my data. This number is somewhat arbitrary so I might revisit later. Lastly, I shuffle the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-chunks.txt', encoding='utf-8')\n",
    "af = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First five Afrikaans chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/en-chunks.txt', encoding='utf-8')\n",
    "en = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 5 English chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the encoding issues from files I showed you mid-pipeline do not seem present in these written out files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-chunks.txt', encoding='utf-8')\n",
    "kaa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weird encoding errors are now gone for Qaraqalpaqsha. Here are its first 5 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaa[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/ml-chunks.txt')\n",
    "ml = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some weird encoding issues with Jupyter specifically. In Malayalam, Jupyter can't display this right: ണ്‌ \n",
    "\n",
    "Even pasting it into this markup cell, it's showing me a red dot next to this character when I'm in edit mode, I guess to say it doesn't want to show the diacritic mark. And in the following code printout, it shows the character as \\u200c instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure how much I can do about that issue; I know the correct \"information\" for that character is in there since these files are displaying correctly on my computer. I've tried reading the file in with different encoding schemes or doing the `str.encode('utf-8').decode('raw_unicode_escape')` trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do (for data preparation)\n",
    "- Anonymize writing system using method described in first progress report (see there for details)\n",
    "- Create corpus that has all languages mixed and shuffled together, with each chunk labeled for its language\n",
    "- Feature extraction \n",
    "- Split into train/dev/test sets, ensuring equal representation of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
