{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proof-spouse",
   "metadata": {},
   "source": [
    "# Data explanation (EXISTING)\n",
    "This is the first notebook in the project\n",
    "\n",
    "In this notebook, the first section explores the dataset and the second section walks through how the dataset was gathered and cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-there",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Intro to the dataset](#Intro-to-the-dataset)\n",
    "    - [The extracted subdirectory](#The-extracted-subdirectory)\n",
    "    - [The texts subdirectory](#The-texts-subdirectory)\n",
    "    - [The chunked subdirectory](#The-chunked-subdirectory)\n",
    "- [Explanation of data gathering pipeline](#Explanation-of-data-gathering-pipeline)\n",
    "    - [1: Networking setup](#1:-Networking-setup)\n",
    "    - [2: Find the data files to download](#2:-Find-the-data-files-to-download)\n",
    "    - [3: Download raw wikipedia XML files](#3:-Download-raw-wikipedia-XML-files)\n",
    "    - [4: Remove the XML and wikipedia formatting](#4:-Remove-the-XML-and-wikipedia-formatting)\n",
    "    - [5: Get the article text](#5:-Get-the-article-text)\n",
    "    - [6: Cleaning to remove punctuation, extra spaces](#6:-Cleaning-to-remove-punctuation,-extra-spaces)\n",
    "    - [7: Chunking and shuffling](#7:-Chunking-and-shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "american-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for file operations\n",
    "import pandas as pd\n",
    "import wikipedia # gets list of all languages on wikipedia\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-somerset",
   "metadata": {},
   "source": [
    "---\n",
    "# Intro to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forbidden-jamaica",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/snc/Documents/wikidata/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-45f07df4e675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/snc/Documents/wikidata/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/snc/Documents/wikidata/'"
     ]
    }
   ],
   "source": [
    "path = '/Users/snc/Documents/wikidata/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-clearing",
   "metadata": {},
   "source": [
    "On the highest level of my data directory, there are three subdirectories. \n",
    "\n",
    "The extracted subdirectory contains the raw text of all articles for all languages' wikipedias, untouched except for being extracted from the XML format of the Wikipedia server dumps. Each language is its own .txt file in this directory. \n",
    "\n",
    "In texts, I have the data once all the articles's text bodies have been concatenated together into one very long string, with punctuation, URLs, formatting characters and so on stripped away. Each language has its own file(s) in the texts subdirectory - some languages' wikipedias were too big for the language to be held in just one file and subsequently tranferred to another computer (I would get memory errors), so many languages are split into many installments (the number is proportionate to the size of that wikipedia). \n",
    "\n",
    "Lastly, the chunked subdirectory stores a subset of the texts data for more convenient use when trying out machine learning models. Here, each language is represented by its own file. I randomly selected 500-character chunks from each language's files in the texts subdirectory, then shuffled the order of the chunks. For languages with more than 10000 contiguous 500-character chunks, I capped the number of chunks at 10000. However, smaller wikipedias were to small too have all 10000 chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-framing",
   "metadata": {},
   "source": [
    "## The extracted subdirectory\n",
    "Here I'll make a dataframe of all languages/files in extracted and the filesize for that language in megabytes (and recall a gigabyte is 1000 megabytes)\n",
    "### The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_list = []\n",
    "for f in os.listdir(path+'extracted/'):\n",
    "    if f.startswith('.'): continue #skip system files\n",
    "    fname = f[:f.index('.')]\n",
    "    lang = wikipedia.languages()[fname]\n",
    "    extracted_list.append((fname, lang, f, os.path.getsize(path+'extracted/' + f)/1000000))\n",
    "files= pd.DataFrame(extracted_list, columns=['code', 'lang', 'fname', 'exfsize_MB'])\n",
    "files.set_index('code', inplace=True)\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-posting",
   "metadata": {},
   "source": [
    "248 files/languages in the extracted subdirectory - each language in the dataset is represented by its own file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.exfsize_MB.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-capability",
   "metadata": {},
   "source": [
    "This subdirectory is taking up about 85000 megabytes or 85 gigabytes on my computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-bracket",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "Here is an example file, for Greek (language code el):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path+'extracted/el.txt', 'r')\n",
    "exel = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exel) # each title/header and each paragraph of each article is its own line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "exel[0:10] # first 10 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-remains",
   "metadata": {},
   "source": [
    "## The texts subdirectory\n",
    "### The files\n",
    "Since languages can be split up across multiple files, the naming scheme is as follows when dividing language with code 'lang' into n parts: the first file is 'lang0.txt', second is 'lang1.txt' up until 'lang(n-1).txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['tf_totalsize_MB'] = 0.0 # total size for each language in texts subdirectory\n",
    "files['tf_count'] = 0 # number of files dedicated to that language\n",
    "for f in os.listdir(path + 'texts/'):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')]\n",
    "    lang = ''.join([i for i in lang if not i.isdigit()]) # remove \n",
    "                                                        #installment number to get lang code\n",
    "    files['tf_count'][lang] += 1\n",
    "    files['tf_totalsize_MB'][lang] += os.path.getsize(path+'texts/'+f)/1000000\n",
    "files['tf_avgsize_MB'] = files['tf_totalsize_MB'] / files['tf_count']\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-accountability",
   "metadata": {},
   "source": [
    "For instance, here is the information for the English dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.loc['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-means",
   "metadata": {},
   "source": [
    "The `tf_count` entry tells you there's 10 files to store the English wikipedia. The total size of these files (from the `tf_totalsize_MB` entry) is 13384 Megabytes or about 13 gigabytes. The average filesize for each of the 10 English files (since I split the data evenly amongst each file) is 1338 Megabytes or 1.3 gigabytes.\n",
    "\n",
    "Now, you might wonder why I was able to create the 14 gigabyte file to represent English in the extracted directory, but I got a memory error when trying to create one 13 gigabyte file to represent English in the texts directory. I have that question too. Maybe it's because the texts files have no linebreaks while the extracted ones do, making the extracted ones easier to split up when writing to file and transferring to another computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['tf_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['tf_totalsize_MB'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-affiliation",
   "metadata": {},
   "source": [
    "The total texts directory contains 567 files (divided by 248 languages is average 2.3 files per languages). The total space taken up by this subdirectory is about 80.5 gigabytes on my computer, so just slightly smaller than the extracted subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-concert",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "This is the first file for German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path + 'texts/de0.txt', 'r')\n",
    "tede = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-actress",
   "metadata": {},
   "source": [
    "The file's contents are just a lot of characters in one continuous line. Other than the very end of the file, the newline character '\\n' is not present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\n' in tede[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-connectivity",
   "metadata": {},
   "source": [
    "First 1000 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "tede[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-truth",
   "metadata": {},
   "source": [
    "## The chunked subdirectory\n",
    "### The files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['fchunk_size_MB'] = 0 # file size of chunked file\n",
    "files['nchunks'] = 0 # number of chunks (lines) in the file\n",
    "for f in os.listdir(path+'chunked/'):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')]\n",
    "    file = open(path+'chunked/'+f, 'r')\n",
    "    chunks = file.readlines()\n",
    "    file.close()\n",
    "    files['nchunks'][lang] = len(chunks)\n",
    "    files['fchunk_size_MB'][lang] = os.path.getsize(path+'chunked/'+f)/1000000\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-bulgaria",
   "metadata": {},
   "source": [
    "As shown in this histogram of the number of chunks per language, most languages have the full 10000 chunks but a non-negligible amount do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['nchunks'].plot.hist(bins=30)\n",
    "plt.savefig('figs/nchunks.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-writing",
   "metadata": {},
   "source": [
    "The average number of chunks is 7719 while the median is 10000. The minimum number of chunks is 343."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['nchunks'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-affiliate",
   "metadata": {},
   "source": [
    "The language with only 343 chunks is Lakku (лакку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[files['nchunks']==343]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-hollow",
   "metadata": {},
   "source": [
    "153 out of the 248 languages (so 62%) have a full 10000 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "files['fchunk_size_MB'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-catalyst",
   "metadata": {},
   "source": [
    "These files take up 1366 megabytes or 1.4 gigabytes on my computer - much smaller than the extracted or texts subdirectories!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-damage",
   "metadata": {},
   "source": [
    "### Example for a language\n",
    "Here are the Turkish chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path+'chunked/tr.txt', 'r')\n",
    "chtr = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-budget",
   "metadata": {},
   "source": [
    "Each line is a chunk. Turkish has 10000 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-albert",
   "metadata": {},
   "source": [
    "Each line is 501 characters long (500 characters plus the newline character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(l) for l in chtr])/len(chtr) # average line length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(l)==501 for l in chtr]) # this is the number of lines in the file that have exactly\n",
    "# 501 characters. you see it's equal to the number of lines in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-torture",
   "metadata": {},
   "source": [
    "These are the first five chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "chtr[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-muscle",
   "metadata": {},
   "source": [
    "---\n",
    "# Explanation of data gathering pipeline\n",
    "Now that the dataset is introduced, I'll explain how I gathered it. To assemble this full, final copy of the dataset, I ran my datagathering script on CRC over a period of days. The script is located in the datagather/crc directory of this repository.\n",
    "\n",
    "I'm not sure how long the script itself would take on its own now because as I ran it, it would crash for various reasons (eg, writing large strings causing memory errors or networking set up wrong) or my session would expire, so I would need to fix the issue and start it running again where it left off. Sometimes I also had up to six copies of the script running at a time over the weekend when CRC had a lot of open resources, each collecting languages starting with a different letter of the alphabet.\n",
    "## 1: Networking setup\n",
    "CRC doesn't have enough storage space for the full dataset, so I needed to periodically send data somewhere else and delete all of one language's files from CRC before moving onto the next language. I would send files to an old laptop I have for storage (see second progress report for more details). \n",
    "\n",
    "Here is an outline of the networking aspects involved in my data gathering/cleaning:\n",
    "\n",
    "Outside script\n",
    "\n",
    "1. Get an internet address (or technically, publically expose port 22) for the storage laptop by running `./ngrok tcp 22` in shell and leaving it running\n",
    "2. On my normal laptop, connect to Pitt's VPN (required to access CRC)\n",
    "2. ssh to crc.h2p.pitt.edu to access CRC\n",
    "3. Pass storage laptop's username, address and port to the script as command line arguments\n",
    "\n",
    "Inside script\n",
    "\n",
    "1. Prompt user for password, store it for future use in the runtime\n",
    "2. Establish a test sftp connection with storage laptop to verify password (if fails, prompt user again)\n",
    "3. After downloading a language's raw dump and extracting XML, send backup to device by opening a new sftp connection, transferring file, then closing connection (forms the extracted subdirectory)\n",
    "4. Transfer another copy of the data after passing through the cleaning steps (forms the texts subdirectory)\n",
    "5. Transfer another copy of the data after the chunking step (forms the chunked subdirectory)\n",
    "\n",
    "What follows is an example that creates a very small exammple helloworld.txt file, then transfers it to another computer. I didn't bother with the VPN or ngrok for this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test file\n",
    "fname = 'helloworld.txt'\n",
    "f = open(fname, 'w')\n",
    "f.writelines('hello world\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'snc'\n",
    "port = 22\n",
    "address = '10.0.0.25' # this is the computer's PRIVATE IP address: the address your internet\n",
    "# router uses to distinguish this computer from any other device on your own network so \n",
    "# you don't recieve all the webpages requested by your brother and grandma or vice versa\n",
    "# you can entering the IP address for your own computer here and use your laptop as the source \n",
    "# and the destination if you want to try it out. Then, it would be a bit like emailing or calling\n",
    "# yourself in that the email is sent to the sender or the call is sent to the caller.\n",
    "\n",
    "# to enable this sftp connection for a mac, go to System Preferences > Sharing > \n",
    "# Enable \"Remote Login\". There, it will display the IP address to use. Your source/local\n",
    "# and destination/remote computers must be on the same wifi network unless you learn to use\n",
    "# ngrok or something similar. Don't know how to do it on other operating systems \n",
    "# but there's probably tutorials on line if it isn't enabled by defailt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = getpass.getpass(prompt='sftp password: ') # password for your username \n",
    "# on destination/remote computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTABLISH CONNECTION\n",
    "client = paramiko.client.SSHClient()\n",
    "client.load_system_host_keys() # this loads any local ssh keys\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "client.connect(address, port=port, username=username, password=pwd)\n",
    "sftp = client.open_sftp() # type SFTPClient\n",
    "\n",
    "# TRANSFER THE FILE\n",
    "sftp.put('./'+fname, fname) #src, dest path/filename\n",
    "print('transferred', fname)\n",
    "\n",
    "# CLOSE THE CONNECTION\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-incident",
   "metadata": {},
   "source": [
    "If you run this yourself, look in your home (~) directory for the transferred file at the destination/remote computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-norman",
   "metadata": {},
   "source": [
    "## 2: Find the data files to download\n",
    "The Wikipedia API offers a dictionary of all the languages of a Wikipedia edition that exists or used to exist. It provides both the language's abbreviation/code and name. So, my code iterates through this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "langsdict = wikipedia.languages()\n",
    "langsdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-monitoring",
   "metadata": {},
   "source": [
    "When I have the language's abbreviation `abv`, I can construct the download link for the wikipedia dump as: \n",
    "\n",
    "'*https://dumps.wikimedia.org/*' + `abv` + '*wiki/latest/*' + `abv` + '*wiki-latest-pages-articles.xml.bz2*'\n",
    "\n",
    "Once I download it, I store it on my computer. Then, I check to see if the file is already present locally before re-downloading. This way I don't need to keep stressing Wikipedia's server each time I run my program. Some wikipedias are closed/no longer maintained and throw an error if I try to download, so I just skip those languages. Also, I deem languages as too small if their raw dump file size is less than 1MB, I'll discuss more why later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-chance",
   "metadata": {},
   "source": [
    "## 3: Download raw wikipedia XML files\n",
    "The script downloads these automatically, they come in a compressed .xml.bz format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpspath = './datagather/dumps/' # where I'm storing the raw files\n",
    "dumpsraw = dict()\n",
    "for f in os.listdir(dumpspath):\n",
    "    if f.startswith('.'): continue\n",
    "    lang = f[:f.index('.')-4] # isolate just name of the language\n",
    "    dumpsraw[lang] = [langsdict[lang], os.path.getsize(dumpspath + f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-appeal",
   "metadata": {},
   "source": [
    "Dataframe of the languages I've gotten - 248 languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps = pd.DataFrame.from_dict(dumpsraw, columns=['name', 'fsize'], orient='index')\n",
    "print(dumps.shape)\n",
    "dumps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-shuttle",
   "metadata": {},
   "source": [
    "This is the number of bytes for these raw dump files alone, which is about 76 gigabytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps['fsize'].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-witness",
   "metadata": {},
   "source": [
    "These are the languages I skipped for either of the reasons I mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = [langsdict[l] for l in langsdict.keys() if l not in dumps.index]\n",
    "print(len(excluded), 'languages excluded:\\n', ', '.join(excluded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-brisbane",
   "metadata": {},
   "source": [
    "But these are the ones I was able to get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dumps.shape[0], 'languages included:\\n', ', '.join(dumps['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-psychology",
   "metadata": {},
   "source": [
    "Largest wikipedias by raw dump file size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps.sort_values('fsize', ascending=False).head(10) # largest wikis included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-portal",
   "metadata": {},
   "source": [
    "English alone is about 18 GB! The distant second is German at about 6 GB.\n",
    "\n",
    "Smallest file sizes that I included (since cutoff was 1MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps.sort_values('fsize').head(10) # smallest wikis included"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-frederick",
   "metadata": {},
   "source": [
    "Plot of the filesizes for the languages I included. You can see most are quite small, while a few are very big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps['fsize'].plot.hist(bins=30) # many small wikis, just a few big wikis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-software",
   "metadata": {},
   "source": [
    "Here is an example of Qaraqalpaqsha's raw dump file, if I manually decompress it (I don't store these anywhere, it's all handled by the extractor tool I use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-raw.xml', 'r')\n",
    "exdump = f.readlines()\n",
    "f.close()\n",
    "exdump[:20] # start of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "exdump[3745:3765] # random middle part showing part of an article "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-sphere",
   "metadata": {},
   "source": [
    "One of the languages that ultimately gets excluded is Qafár af. Here I'll read in its expanded dump file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/aa-raw.xml')\n",
    "aaraw = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaraw[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aaraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-humidity",
   "metadata": {},
   "source": [
    "At first glance, this language's XML file has 5526 lines, which sounds like enough to at least get a good sampling of what the language looks like. But then I stripped all the XML and extra Wikipedia information off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-engagement",
   "metadata": {},
   "source": [
    "## 4: Remove the XML and wikipedia formatting\n",
    "Wikipedia formatting is stuff like the particular templates used in the articles, the random stuff you see at the very start of the above raw XML file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/aa-extracted.txt')\n",
    "aa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-performer",
   "metadata": {},
   "source": [
    "Here is Qafár af (the example excluded language)'s *entire* file without the XML and extra Wikipedia formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-albania",
   "metadata": {},
   "source": [
    "As you can see, the output of the tool I used is one json object per page/article. From 5526 lines, Qafár af is left with just two pages, one of which has no text at all. The other page is just a sentence long; it seems to be a message notifying users that this wikipedia is closed and not maintained. This is why I excluded languages whose raw dump files are smaller than 1MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-responsibility",
   "metadata": {},
   "source": [
    "Returning to Qaraqalpaqsha, which I showed the raw dump for earlier, here is its Wikipedia with the XML and Wikipedia formatting removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-extracted.txt', 'r')\n",
    "kaa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaa[:3] # first 3 json objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-activation",
   "metadata": {},
   "source": [
    "The encoding is causing some weird display issues for some characters here, it's strange because it shows up fine in my text editor and in zsh with the `head` command. Either way, it gets corrected once it travels through the rest of my current data correction pipeline and the final file for this language which I'll show later displays fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-meditation",
   "metadata": {},
   "source": [
    "The tool I'm using to go from raw dump to \"clean\" json items is called WikiExtractor. The current version on GitHub is not stable, but it's here: https://github.com/attardi/wikiextractor\n",
    "\n",
    "I forked it and made a stable version here: https://github.com/soCromp/wikiextractor\n",
    "\n",
    "I needed to use specifically this tool for another project I'm doing in another class and spent a lot of time fixing it and figuring out how to run it. It's designed specifically for processing Wikipedia data en masse and already \"knows\" Wikipedia formatting and how to find certain article attributes. As a result, I decided to just use it again here rather than learn how to use totally another thing like Beautiful Soup - I've used their Java library a couple years ago (jsoup) and remembered taking a while to get data cleaned like I wanted. \n",
    "\n",
    "The tool runs pretty quick - Afrikaans takes 1-2 minutes and English maybe 5. I also pass the text through some shell regex like `sed` at this point for a little extra cleaning. Shell regex seems to run faster than Python regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-wyoming",
   "metadata": {},
   "source": [
    "## 5: Get the article text\n",
    "The next step in the pipeline is doing some encoding correction after the XML and Wikipedia formatting is extracted. Then, I grab just article text from the json objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-articles.txt')\n",
    "aftexts = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(aftexts[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-luther",
   "metadata": {},
   "source": [
    "As you can see, it still has line breaks, punctuation, numbers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-intensity",
   "metadata": {},
   "source": [
    "After this step, I was planning to tokenize. I added this in and started to run the script on all the data, but disliked the results I was getting. I don't know what tool I could use to tokenize different languages since there are so many different rules and probably a lot of things I don't know about how to tokenize the languages I don't speak. So, I just took the tokenizing step out at least for now.\n",
    "## 6: Cleaning to remove punctuation, extra spaces\n",
    "Next, I just pass the text through a filter to remove all puntuation characters and replace sequences of multiple spaces in a row with just one space. I use a special unicode library function, unicodedata.category() to detect any unicode puntuation character in order to catch even puntuation like the Japanese period \"。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-text.txt')\n",
    "afclean = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "afclean[0][:402]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-toner",
   "metadata": {},
   "source": [
    "Again, there is some strange encoding issue when I display it in Jupyter. You can see it in the spaces between where the numbers were. It doesn't occur when I display it in Atom, Sublime or TextEdit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-compact",
   "metadata": {},
   "source": [
    "## 7: Chunking and shuffling\n",
    "Originally I had the idea to make each line a sentence or a certain number of words. Then I realized not all languages separate words or sentences the same way, and decided the most language-neutral way to do this would be to simply make each line 500 characters long. I refer to each line as a chunk, and simply divide the long block of text every 500 characters. I'm sure there are better ways to handle this, but I chose this simple way for now at least. Then, if there are more than 100000 chunks, I randomly sample 100000 chunks to prevent any larger languages from being overrepresented in my data. This number is somewhat arbitrary so I might revisit later. Lastly, I shuffle the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/af-chunks.txt', encoding='utf-8')\n",
    "af = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-assistant",
   "metadata": {},
   "source": [
    "First five Afrikaans chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "af[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/en-chunks.txt', encoding='utf-8')\n",
    "en = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-philip",
   "metadata": {},
   "source": [
    "First 5 English chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-rebound",
   "metadata": {},
   "source": [
    "Some of the encoding issues from files I showed you mid-pipeline do not seem present in these written out files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/kaa-chunks.txt', encoding='utf-8')\n",
    "kaa = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-stability",
   "metadata": {},
   "source": [
    "The weird encoding errors are now gone for Qaraqalpaqsha. Here are its first 5 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaa[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data_samples/ml-chunks.txt')\n",
    "ml = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-planner",
   "metadata": {},
   "source": [
    "There are still some weird encoding issues with Jupyter specifically. In Malayalam, Jupyter can't display this right: ണ്‌ \n",
    "\n",
    "Even pasting it into this markup cell, it's showing me a red dot next to this character when I'm in edit mode, I guess to say it doesn't want to show the diacritic mark. And in the following code printout, it shows the character as \\u200c instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-picture",
   "metadata": {},
   "source": [
    "I'm not sure how much I can do about that issue; I know the correct \"information\" for that character is in there since these files are displaying correctly on my computer. I've tried reading the file in with different encoding schemes or doing the `str.encode('utf-8').decode('raw_unicode_escape')` trick.\n",
    "\n",
    "As much as I want to fix it, it potentially is a deeply embedded issue in my browser or Jupyter or even my operating system. Knowing there's plenty of other stuff to do with the project, it's probably important that I prioritize at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
